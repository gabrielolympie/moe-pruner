{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba9ea0ed-f3df-4aa9-9355-7622e65090cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from memory_utils import load_module_weights_and_freeze_optimized, memory_cleanup\n",
    "from accelerate import init_empty_weights\n",
    "from deepseek_v3.modeling_deepseek import DeepseekV3ForCausalLM, DeepseekV3MoE\n",
    "import torch\n",
    "import json\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import AutoConfig, AutoModelForCausalLM\n",
    "\n",
    "from fp8_linear import FP8Linear\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ec49a97-d39c-4f8e-a704-eb62b3c02582",
   "metadata": {},
   "source": [
    "## Instantiate empty model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08388ad5-eb8e-49a7-b919-d1ae96c0f1dd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_name=\"deepseek_v3\"\n",
    "\n",
    "config = AutoConfig.from_pretrained(\n",
    "    model_name,\n",
    "    trust_remote_code=True,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    attn_implementation=\"flash_attention_2\",\n",
    ")\n",
    "\n",
    "with init_empty_weights():\n",
    "    model = DeepseekV3ForCausalLM(config)\n",
    "\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b8b9c9e-3ff5-461f-b954-2e85eaae4cbc",
   "metadata": {},
   "source": [
    "## Load non expert weights in model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27ff400e-e778-4586-9cf1-06a2577d8e72",
   "metadata": {},
   "source": [
    "This part should take about 16go of VRAM (base model weight without experts is about 17B parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "580b23e4-3cb3-416a-8830-9d6740e31fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"{model_name}/model.safetensors.index.json\", 'r') as f:\n",
    "    weight_map = json.load(f)['weight_map']\n",
    "\n",
    "ignore=[\"gate.weight\", \".experts.\"]\n",
    "device=\"cuda:0\"\n",
    "\n",
    "model = load_module_weights_and_freeze_optimized(\n",
    "    model, \n",
    "    module_path=None,\n",
    "    weight_map=weight_map,\n",
    "    model_name=model_name,\n",
    "    max_workers=16,\n",
    "    ignore=ignore,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "memory_cleanup()\n",
    "\n",
    "from memory_utils import count_parameters\n",
    "count_parameters(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02acc397-b4ba-415f-b294-b6a1f4f12ea8",
   "metadata": {},
   "source": [
    "## Cast quantized layers back to bfloat16, and offload them to cpu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0486a230-8727-45ed-9412-992e33957731",
   "metadata": {},
   "source": [
    "Casting the model to bf16 will push VRAM usage to around 28go, a bit to much for my tiny 3090"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5e84116-8633-4e45-9143-6255caf4af19",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, module in tqdm(model.named_modules(), desc=\"Replacing FP8Linear\"):\n",
    "    if isinstance(module, FP8Linear):\n",
    "        # Split the name into its components\n",
    "        parts = name.split('.')\n",
    "        parent = model\n",
    "        # Traverse the model hierarchy to get the parent module\n",
    "        for part in parts[:-1]:\n",
    "            if part.isdigit(): #handle sequential modules\n",
    "                parent = parent[int(part)]\n",
    "            elif '[' in part: # handle ModuleDict\n",
    "                parent = parent[part.split('[')[0]]\n",
    "                idx = part.split('[')[1].split(']')[0].replace(\"'\",'').replace('\"','') #get string\n",
    "                parent = parent[idx] #now get the actual submodule\n",
    "\n",
    "\n",
    "            else:\n",
    "                parent = getattr(parent, part)\n",
    "\n",
    "        # Replace the module using setattr on the parent\n",
    "        setattr(parent, parts[-1], module.to_linear().to('cpu'))\n",
    "memory_cleanup()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "176c113c-d142-4b49-93b7-eb17c57c36ce",
   "metadata": {},
   "source": [
    "## Load distilled experts weights and save to disk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23977449-2582-45ca-8de4-8825815c5069",
   "metadata": {},
   "source": [
    "Depending on expert size, vram can vary. You can load some on gpu and some on cpu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a285ef25-3458-4ace-bdeb-7960f88576e0",
   "metadata": {},
   "source": [
    "You shall witness the magic moment of parameter shrinking as the layers are progressively updated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59a304b7-85d4-4ef5-abcc-dddee1d9e459",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f2913da-4600-4617-96fd-6200eb9d6267",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "config=model.config\n",
    "source_folder=model_name\n",
    "mlp_params = [\n",
    "    # (22,6),\n",
    "    # (16,4),\n",
    "    # (8,2),\n",
    "    (4,1),\n",
    "]\n",
    "\n",
    "for specs in mlp_params:\n",
    "    config.n_routed_experts = specs[0]\n",
    "    config.num_experts_per_tok = specs[1]\n",
    "    model.config=config\n",
    "    base_path=f\"DeepSeek-V3_{specs[0]}@{specs[1]}/\"\n",
    "    \n",
    "    mlp = DeepseekV3MoE(config).to('cuda:1')\n",
    "    for layer_index in tqdm(range(3,61)):\n",
    "        mlp.gate.load_state_dict(torch.load(base_path+f\"gate_layer_{layer_index}.ckpt\"))\n",
    "        mlp.experts.load_state_dict(torch.load(base_path+f\"experts_layer_{layer_index}.ckpt\"))\n",
    "        \n",
    "        model.model.layers[layer_index].mlp.gate=deepcopy(mlp.gate).to(dtype=torch.bfloat16, device=\"cpu\")\n",
    "        model.model.layers[layer_index].mlp.experts=deepcopy(mlp.experts).to(dtype=torch.bfloat16, device=\"cpu\")\n",
    "        \n",
    "        count_parameters(model)\n",
    "        memory_cleanup()\n",
    "    \n",
    "    \n",
    "    model_checkpoint_name=f\"DeepSeek-V3-{specs[0]}@{specs[1]}-unhealed\"\n",
    "    model.save_pretrained(model_checkpoint_name)\n",
    "    \n",
    "    files_to_copy = [\n",
    "        \"modeling_deepseek.py\",\n",
    "        \"tokenizer.json\",\n",
    "        \"tokenizer_config.json\",\n",
    "    ]\n",
    "    \n",
    "    # Create the destination directory if it doesn't exist\n",
    "    if not os.path.exists(model_checkpoint_name):\n",
    "        os.makedirs(model_checkpoint_name)\n",
    "    \n",
    "    for file in files_to_copy:\n",
    "        source_path = os.path.join(source_folder, file)\n",
    "        destination_path = os.path.join(model_checkpoint_name, file)\n",
    "    \n",
    "        # Check if the source file exists before copying\n",
    "        if os.path.exists(source_path):\n",
    "            shutil.copy2(source_path, destination_path)  # Use copy2 to preserve metadata\n",
    "            print(f\"Copied '{file}' from '{source_folder}' to '{model_checkpoint_name}'\")\n",
    "        else:\n",
    "            print(f\"Warning: File '{file}' not found in '{source_folder}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51d7f901-12c1-4ed6-8f54-654b9ead1933",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
