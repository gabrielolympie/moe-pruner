{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "56d7c06a-3a15-45f9-8674-ca79ba0ca52b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "    \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping, Callback, SpikeDetection\n",
    "from pytorch_lightning.loggers import TensorBoardLogger \n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoConfig,\n",
    "    AutoModelForCausalLM,\n",
    "    BitsAndBytesConfig,\n",
    "    get_linear_schedule_with_warmup,\n",
    "    HqqConfig\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model\n",
    "# from ademamix import AdEMAMix\n",
    "import bitsandbytes as bnb\n",
    "from bitsandbytes.optim.ademamix import AdEMAMix8bit as AdEMAMix\n",
    "from bitsandbytes.optim.adamw import AdamW8bit as AdamW\n",
    "\n",
    "from datasets import load_dataset\n",
    "import os\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import gc\n",
    "import argparse\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from torch_utils import memory_cleanup, count_parameters\n",
    "from liger_kernel.transformers import apply_liger_kernel_to_llama\n",
    "\n",
    "from pytorch_lightning.strategies import FSDPStrategy\n",
    "import torch.distributed.fsdp.fully_sharded_data_parallel as fsdp\n",
    "from torch.distributed.fsdp import MixedPrecision\n",
    "\n",
    "from functools import partial\n",
    "from fsdp_utils import fsdp_hqq_dora_model_for_causal_lm, get_wrapping_policy\n",
    "from torch.distributed.fsdp.api import BackwardPrefetch, CPUOffload, ShardingStrategy\n",
    "\n",
    "from torch.distributed.fsdp import FullyShardedDataParallel as FSDP\n",
    "import torch.distributed as dist\n",
    "from model_utils import rsetattr\n",
    "\n",
    "class HealingDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, max_length=512):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.data[idx]\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            max_length=self.max_length,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        return {\n",
    "            \"input_ids\": encoding[\"input_ids\"].squeeze(),\n",
    "            \"attention_mask\": encoding[\"attention_mask\"].squeeze(),\n",
    "        }\n",
    "\n",
    "def load_and_prepare_data(tokenizer, batch_size=8, max_length=512, num_workers=os.cpu_count(), train_sample_limit=None, val_sample_limit=None):\n",
    "    dataset = load_dataset(\n",
    "        \"cognitivecomputations/dolphin-r1\", \"nonreasoning\", cache_dir=\"../dolphin-r1\"\n",
    "    )[\"train\"]\n",
    "\n",
    "    # Apply sample limits if provided\n",
    "    if train_sample_limit is not None:\n",
    "        train_dataset = dataset.select(range(train_sample_limit))  # Use .select for efficiency\n",
    "    else:\n",
    "        train_dataset = dataset\n",
    "\n",
    "    if val_sample_limit is not None:\n",
    "        val_dataset = dataset.select(range(train_sample_limit, train_sample_limit+val_sample_limit)) # Use .select for efficiency\n",
    "    else:\n",
    "        val_dataset = dataset\n",
    "\n",
    "    train_dataset = train_dataset[\"messages\"]\n",
    "    val_dataset = val_dataset[\"messages\"]\n",
    "    \n",
    "    train_dataset = [\n",
    "        tokenizer.apply_chat_template(elt, tokenize=False, add_generation_prompt=False)\n",
    "        for elt in tqdm(train_dataset, desc=\"Preparing dataset train\")\n",
    "    ]\n",
    "\n",
    "    val_dataset = [\n",
    "        tokenizer.apply_chat_template(elt, tokenize=False, add_generation_prompt=False)\n",
    "        for elt in tqdm(val_dataset, desc=\"Preparing dataset train\")\n",
    "    ]\n",
    "\n",
    "    train_dataset = HealingDataset(\n",
    "        train_dataset, tokenizer, max_length=max_length\n",
    "    )\n",
    "    val_dataset = HealingDataset(\n",
    "        val_dataset, tokenizer, max_length=max_length\n",
    "    )\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers, pin_memory=True\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers, pin_memory=True\n",
    "    )\n",
    "\n",
    "    return train_loader, val_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eec67bc-4405-4685-80ac-53c9222cb9bc",
   "metadata": {},
   "source": [
    "## Load Base Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4e19eac-56b7-40d2-94ea-90533abb4bcb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d955067ee7944ebbb35305004eddfdf0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Preparing dataset train:   0%|          | 0/32000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d09cfecc56d4dc7a858d89ff42ba122",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Preparing dataset train:   0%|          | 0/512 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading HF Model to CPU\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24c7e37e31b944d3aee7170028641621",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantizing linear layers\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bac03369e1884d669fc3a82a3113562c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing modules:   0%|          | 0/3537 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"0\"\n",
    "os.environ[\"TORCHDYNAMO_DISABLE_GRAPH_CAPTURE\"] = \"1\"\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:128\"\n",
    "\n",
    "torch.set_float32_matmul_precision('medium')\n",
    "# torch.backends.cuda.enable_flash_sdp(True)\n",
    "# torch.backends.cuda.enable_mem_efficient_sdp(True)\n",
    "\n",
    "weights_location=\"deepseek_v3\"\n",
    "n_routed_experts=8\n",
    "n_active_experts=4\n",
    "epochs=1\n",
    "batch_size=1\n",
    "max_length=32\n",
    "\n",
    "learning_rate=3e-5\n",
    "train_sample_limit=32000\n",
    "val_sample_limit=512\n",
    "warmup_steps=128\n",
    "compilation=False\n",
    "checkpoint_every_n_steps=1024\n",
    "accumulate_grad_batches=1\n",
    "\n",
    "\n",
    "model_name=f\"/home/golympie/ai-toolbox/{weights_location}_{n_routed_experts}a{n_active_experts}\" ## i displaced the model on a faster disc for increased loading speed.\n",
    "\n",
    "log_name=f\"{weights_location}_{n_routed_experts}a{n_active_experts}\"\n",
    "log_dir=\"pl_logs\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"deepseek-ai/DeepSeek-V3\", trust_remote_code=True\n",
    ")\n",
    "\n",
    "# Load and prepare data\n",
    "train_loader, val_loader = load_and_prepare_data(\n",
    "    tokenizer, batch_size=batch_size, max_length=max_length,\n",
    "    train_sample_limit=train_sample_limit, val_sample_limit=val_sample_limit\n",
    ")\n",
    "\n",
    "memory_cleanup()\n",
    "\n",
    "# Calculate total steps for the scheduler\n",
    "total_steps = len(train_loader) * epochs\n",
    "\n",
    "target_modules=[]\n",
    "for i in range(n_routed_experts):\n",
    "    target_modules.append(f\"mlp.experts.{i}.gate_proj\")\n",
    "    target_modules.append(f\"mlp.experts.{i}.up_proj\")\n",
    "    target_modules.append(f\"mlp.experts.{i}.down_proj\")\n",
    "\n",
    "target_modules.append('mlp.gate.weight')\n",
    "\n",
    "model=fsdp_hqq_dora_model_for_causal_lm(\n",
    "    model_name,\n",
    "    target_modules=target_modules,\n",
    "    lora_rank=4,\n",
    "    lora_alpha=4,\n",
    "    lora_dropout=0.1,\n",
    "    n_workers=8\n",
    ")\n",
    "\n",
    "memory_cleanup()\n",
    "count_parameters(model)\n",
    "\n",
    "config_dict = model.config.to_dict()\n",
    "\n",
    "if compilation:\n",
    "    print('compile model')\n",
    "    model = torch.compile(model)\n",
    "    \n",
    "memory_cleanup()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46cd0f46-3c72-4505-8496-171f33b7fef6",
   "metadata": {},
   "source": [
    "## Splitting layer between cuda 0 and cuda 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a776231a-4009-4fe8-b4b9-716856b9b3e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from hqq.core.quantize import BaseQuantizeConfig, HQQBackend, HQQLinear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68ba8a6c-9867-4ac6-a059-6fadc6332e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda_1_layer = [f\"layers.{i}\" for i in range(10,42)]\n",
    "\n",
    "for name, parameter in tqdm(model.named_parameters()):\n",
    "    rsetattr(model, name, torch.nn.Parameter(parameter.to('cpu')))\n",
    "\n",
    "for name, module in tqdm(model.named_modules()):\n",
    "    if isinstance(module, HQQLinear):\n",
    "        module.device=\"cpu\"\n",
    "        module.meta['scale']= module.meta['scale'].to('cpu')\n",
    "        module.meta['zero']= module.meta['zero'].to('cpu')\n",
    "\n",
    "model=model.to('cpu')\n",
    "memory_cleanup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff4ae5ce-4ccf-4e4a-831d-8a098aeadf87",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory_cleanup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c95b22c-0baa-428c-b495-9cec537fa779",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cuda_1_layer = [f\"layers.{i}\" for i in range(10,45)]\n",
    "\n",
    "for name, parameter in tqdm(model.named_parameters()):\n",
    "    cond=False\n",
    "    for elt in cuda_1_layer:\n",
    "        if elt in name:\n",
    "            cond=True\n",
    "    \n",
    "    if cond:\n",
    "        rsetattr(model, name, torch.nn.Parameter(parameter.to('cuda:1')))\n",
    "    else:\n",
    "        rsetattr(model, name, torch.nn.Parameter(parameter.to('cuda:0')))\n",
    "\n",
    "for name, module in tqdm(model.named_modules()):\n",
    "    if isinstance(module, HQQLinear):\n",
    "        cond=False\n",
    "        for elt in cuda_1_layer:\n",
    "            if elt in name:\n",
    "                cond=True\n",
    "        if cond:\n",
    "            module.device=\"cuda:1\"\n",
    "            module.meta['scale']= module.meta['scale'].to('cuda:1')\n",
    "            module.meta['zero']= module.meta['zero'].to('cuda:1')\n",
    "        else:\n",
    "            module.device=\"cuda:0\"\n",
    "            module.meta['scale']= module.meta['scale'].to('cuda:0')\n",
    "            module.meta['zero']= module.meta['zero'].to('cuda:0')\n",
    "memory_cleanup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "739840ce-8cd1-494c-93f7-84293f306828",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory_cleanup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "283b1d81-9407-4045-a7f4-6b44d0702c85",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "text=\"this is a very interesting text, mmmmmm\"\n",
    "\n",
    "max_length=256\n",
    "encoding = tokenizer(\n",
    "    [text],\n",
    "    max_length=max_length,\n",
    "    padding=\"max_length\",\n",
    "    truncation=True,\n",
    "    return_tensors=\"pt\",\n",
    ").to('cuda:0')\n",
    "\n",
    "# encoding['attention_mask']=None\n",
    "\n",
    "output=model(\n",
    "    input_ids=encoding['input_ids'],\n",
    "    labels=encoding['input_ids'],\n",
    "    attention_mask=encoding['attention_mask'],\n",
    "    use_cache=False,\n",
    "    output_attentions=False,\n",
    "    output_hidden_states=False\n",
    ")\n",
    "\n",
    "loss=output.loss\n",
    "\n",
    "del output\n",
    "memory_cleanup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d59cb72-7a95-43e8-8ce8-0c9e7b6230cf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
