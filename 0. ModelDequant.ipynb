{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d842eade-c8fd-4151-aa7c-90efbc113f3a",
   "metadata": {},
   "source": [
    "## Ressources"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85c5d3fe-04de-4a14-839b-07213c295466",
   "metadata": {},
   "source": [
    "The pipeline was optimized for the following config:\n",
    "- Storage : 2To SSD @512Mo/s\n",
    "- RAM : 128go DDR4 @3600\n",
    "- CPU : Ryzen 9 3950X 16@32 cores\n",
    "- GPU : 2x RTX 3090, aggregated 48gb DDR6X Vram\n",
    "\n",
    "Hence i can  not guarantee that it will work properly on more frugal hardware.\n",
    "Plus the GPU are not NVLink unified, so some optimisation involve manual allocation to one or the other GPU."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2b9e98c-9fc8-41d9-9f37-3a7a8170dbdf",
   "metadata": {},
   "source": [
    "## Compute the number of parameters for different pruning size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "18bba445-5354-47ec-9c30-32ef1d8627a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_num_parameters(\n",
    "    n_routed_experts,\n",
    "    num_experts_per_tok,\n",
    "):\n",
    "    num_hidden_layers=61\n",
    "    first_k_dense_replace = 3\n",
    "    num_moe_layer = num_hidden_layers - first_k_dense_replace\n",
    "    \n",
    "    hidden_size=7168\n",
    "    intermediate_size=18432\n",
    "    moe_intermediate_size=2048\n",
    "    \n",
    "    \n",
    "    num_heads  = 128\n",
    "    q_lora_rank = 1536\n",
    "    qk_nope_head_dim = 128\n",
    "    qk_rope_head_dim = 64\n",
    "    kv_lora_rank = 512\n",
    "    v_head_dim=128\n",
    "    \n",
    "    n_shared_experts=1\n",
    "    \n",
    "    vocab_size = 129280\n",
    "    \n",
    "    gate_size = n_routed_experts * hidden_size\n",
    "    \n",
    "    mlp_weights = 3 * hidden_size * intermediate_size\n",
    "    moe_mlp_weights = 3 * hidden_size * moe_intermediate_size\n",
    "    \n",
    "    moe_total_weight = n_routed_experts * moe_mlp_weights\n",
    "    moe_active_weight = num_experts_per_tok * moe_mlp_weights\n",
    "    \n",
    "    q_head_dim = qk_nope_head_dim + qk_rope_head_dim\n",
    "    q_a_proj = hidden_size * q_lora_rank + q_lora_rank * q_head_dim\n",
    "    kv_a_proj_with_mqa = hidden_size * (kv_lora_rank  + qk_rope_head_dim) + kv_lora_rank * (num_heads * (q_head_dim - qk_rope_head_dim  + v_head_dim))\n",
    "    o_proj_weight = num_heads * v_head_dim * hidden_size\n",
    "    attention_weight = q_a_proj + 2 * kv_a_proj_with_mqa + o_proj_weight\n",
    "    \n",
    "    base_weight_per_moe_layer = attention_weight + n_shared_experts * moe_mlp_weights + gate_size\n",
    "    base_weight_per_mlp_layer = attention_weight + mlp_weights\n",
    "    \n",
    "    base_model_weight = base_weight_per_moe_layer * num_moe_layer + base_weight_per_mlp_layer * first_k_dense_replace + 2 * vocab_size * hidden_size\n",
    "    \n",
    "    total_expert_weight = moe_total_weight * num_moe_layer\n",
    "    active_expert_weight = moe_active_weight * num_moe_layer\n",
    "    \n",
    "    active_model_weight = active_expert_weight + num_moe_layer + base_model_weight\n",
    "    total_model_weight = total_expert_weight + num_moe_layer + base_model_weight\n",
    "    \n",
    "    print(f\"{n_routed_experts} @ {num_experts_per_tok} => {int(round(total_model_weight/1e9,0))}B @ {int(round(active_model_weight/1e9,0))}B parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5796530c-8061-4803-a7a2-077cac75ed0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "256 @ 8 => 670B @ 37B parameters\n"
     ]
    }
   ],
   "source": [
    "n_routed_experts=256\n",
    "num_experts_per_tok=8\n",
    "calc_num_parameters(n_routed_experts, num_experts_per_tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a54c46a-bbdb-4048-b616-d17e8c86b821",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = [\n",
    "    (256,8),\n",
    "    (22,8),\n",
    "    (16,8),\n",
    "    (8,8),\n",
    "]\n",
    "\n",
    "for elt in p:\n",
    "    calc_num_parameters(*elt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d69d425-c1fb-4289-b6e9-556e43edaae0",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11abf472-db10-4106-9224-1ead46911ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import torch\n",
    "from accelerate import init_empty_weights\n",
    "from accelerate.utils import load_offloaded_weight\n",
    "import json\n",
    "from accelerate import load_checkpoint_in_model, dispatch_model\n",
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "import gc\n",
    "import _pickle as pickle\n",
    "import os\n",
    "# from Distiller import MOEDistiller, count_parameters\n",
    "\n",
    "from transformers import BitsAndBytesConfig\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, GenerationConfig, AutoConfig\n",
    "\n",
    "import torch\n",
    "\n",
    "from memory_utils import load_module_weights_and_freeze_optimized, load_weight_cached, destruct_module_optimized\n",
    "from Distiller import load_model_config,create_empty_model,create_empty_layer, create_empty_layer_fp8\n",
    "\n",
    "from liger_kernel.transformers import apply_liger_kernel_to_llama\n",
    "\n",
    "from copy import deepcopy\n",
    "from Distiller import MOEDistillerV3\n",
    "import os\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm.auto import tqdm\n",
    "import torch\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "import os\n",
    "from modeling_deepseek import _prepare_4d_causal_attention_mask\n",
    "import bitsandbytes as bnb\n",
    "\n",
    "base_path = '/home/golympie/data/'\n",
    "PICKLE_DIR = \"intermediate_states\"\n",
    "\n",
    "# Create directory for pickle files\n",
    "os.makedirs(PICKLE_DIR, exist_ok=True)\n",
    "\n",
    "# # ## Load\n",
    "weight_map, config = load_model_config(\"deepseek_v3\")\n",
    "weight_file = weight_map['model.embed_tokens.weight']\n",
    "\n",
    "\n",
    "apply_liger_kernel_to_llama()\n",
    "\n",
    "model_name = \"DeepSeek-V3\"\n",
    "offload_folder = model_name+'_offload/'\n",
    "output_directory = model_name+'_runner_output/'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"deepseek-ai/DeepSeek-V3\", trust_remote_code=True)\n",
    "\n",
    "def memory_cleanup():\n",
    "    \"\"\"Perform thorough memory cleanup\"\"\"\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.synchronize()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c235aba0-5544-40b3-bf01-ec0376be3f35",
   "metadata": {},
   "source": [
    "## Save layers to disk for easier loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80caaa6a-f8f8-447f-9c11-cc67d33ef680",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# os.makedirs(\"layers\", exist_ok=True)\n",
    "# model = create_empty_model(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bc7e256-e7b8-465a-a877-401bf43e4850",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Embed\n",
    "# model.model.embed_tokens = load_module_weights_and_freeze_optimized(\n",
    "#     model.model.embed_tokens,\n",
    "#     f\"model.embed_tokens\",\n",
    "#     weight_map,\n",
    "#     \"deepseek_v3\",\n",
    "#     max_workers=32,\n",
    "#     fp8_format=\"e4m3\",\n",
    "# )\n",
    "\n",
    "# torch.save(model.model.embed_tokens.state_dict(), 'layers/embed_tokens.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea07c96c-42ed-4dcb-9fa9-d0cc58a56d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## End norm\n",
    "# model.model.norm = load_module_weights_and_freeze_optimized(\n",
    "#     model.model.norm,\n",
    "#     f\"model.norm\",\n",
    "#     weight_map,\n",
    "#     \"deepseek_v3\",\n",
    "#     max_workers=32,\n",
    "#     fp8_format=\"e4m3\",\n",
    "# )\n",
    "\n",
    "# torch.save(model.model.norm.state_dict(), 'layers/norm.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90b78c02-1454-4889-b6f2-8c99c6f1a4ec",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ## Lm head\n",
    "# model.lm_head = load_module_weights_and_freeze_optimized(\n",
    "#     model.lm_head,\n",
    "#     f\"lm_head\",\n",
    "#     weight_map,\n",
    "#     \"deepseek_v3\",\n",
    "#     max_workers=32,\n",
    "#     fp8_format=\"e4m3\",\n",
    "# )\n",
    "\n",
    "# torch.save(model.lm_head.state_dict(), 'layers/lm_head.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70a0bd00-bdd9-491a-9820-8b1d068b9b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# destruct_module_optimized(model)\n",
    "# memory_cleanup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66ffa4c9-d510-4c94-8a04-3e680f859ec1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ## Layers\n",
    "# for i in tqdm(range(61,62)):\n",
    "    \n",
    "#     layer = create_empty_layer(config, layer_idx=i)\n",
    "#     layer = load_module_weights_and_freeze_optimized(\n",
    "#         layer,\n",
    "#         f\"model.layers.{i}\",\n",
    "#         weight_map,\n",
    "#         \"deepseek_v3\",\n",
    "#         max_workers=16,\n",
    "#         fp8_format=\"e4m3\",\n",
    "#     )\n",
    "#     memory_cleanup()\n",
    "\n",
    "#     torch.save(layer.state_dict(), f'./layers/layer_{i}.pt')\n",
    "#     destruct_module_optimized(layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d6c541d-9c4a-4986-a90f-b787f7d614ef",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "## Create dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e402ac6c-0857-454c-806e-d9ce529834e2",
   "metadata": {},
   "source": [
    "The dataset is a small selection of the excellent dolphin r1 dataset because it contains both non reasoning and reasoning sample.\n",
    "\n",
    "Feel free to change the dataset or to scale the approach as you feel (will take longer but with better result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ded31e33-a951-4ad3-a687-58336594c58c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import pickle\n",
    "from tqdm.auto import tqdm\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from dataclasses import dataclass\n",
    "from transformers import AutoTokenizer\n",
    "from typing import Optional, List\n",
    "\n",
    "from Distiller import DistillationConfig\n",
    "\n",
    "@dataclass\n",
    "class PathConfig:\n",
    "    model_name: str = \"deepseek\"\n",
    "    base_dir: str = \"distillation_runs\"\n",
    "    checkpoint_dir: str = \"layers\"\n",
    "    intermediate_dir: str = \"intermediate_states\"\n",
    "    log_dir: str = \"distillation_logs\"\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        # Create all necessary directories\n",
    "        for dir_name in [self.base_dir, self.log_dir, self.intermediate_dir]:\n",
    "            os.makedirs(dir_name, exist_ok=True)\n",
    "            \n",
    "    def get_layer_path(self, layer_idx: int) -> str:\n",
    "        return os.path.join(self.checkpoint_dir, f\"layer_{layer_idx}.ckpt\")\n",
    "    \n",
    "    def get_intermediate_path(self, layer_idx: int, batch_idx: int) -> str:\n",
    "        os.makedirs(os.path.join(self.intermediate_dir, f\"layer_{layer_idx}\"), exist_ok=True)\n",
    "        return os.path.join(self.intermediate_dir, f\"layer_{layer_idx}\",f\"batch{batch_idx}.pt\")\n",
    "    \n",
    "    def get_distillation_path(self, n_experts: int, n_active: int) -> str:\n",
    "        return os.path.join(self.base_dir, f\"{self.model_name}_{n_experts}@{n_active}\")\n",
    "\n",
    "def save_intermediate_state(path_config: PathConfig, layer_idx: int, batch_idx: int, state: torch.Tensor):\n",
    "    \"\"\"Save intermediate layer output to a file in FP8 format\"\"\"\n",
    "    # Downcast to torch.float8_e4m3fn\n",
    "    fp8_tensor = state.to(torch.float8_e4m3fn)\n",
    "    torch.save(fp8_tensor, path_config.get_intermediate_path(layer_idx, batch_idx))\n",
    "\n",
    "def load_intermediate_state(path_config: PathConfig, layer_idx: int, batch_idx: int) -> torch.Tensor:\n",
    "    \"\"\"Load intermediate layer output from a file and upcast from FP8\"\"\"\n",
    "    fp8_tensor = torch.load(path_config.get_intermediate_path(layer_idx, batch_idx))\n",
    "    # Upcast to torch.bfloat16\n",
    "    return fp8_tensor.to(torch.bfloat16)\n",
    "\n",
    "@dataclass\n",
    "class DistillationParams:\n",
    "    n_batch: int = 512\n",
    "    batch_size: int = 4\n",
    "    max_length: int = 512\n",
    "    n_epoch: int = 1\n",
    "    gradient_accumulation_steps: int = 4\n",
    "    calibration_batches: int = 64\n",
    "    learning_rate: float = 1e-4\n",
    "    temperature: float = 1.0\n",
    "    lora_rank: int = 16\n",
    "    lora_alpha: int = 16\n",
    "    max_workers: int = 16\n",
    "    fp8_format: str = \"e4m3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "598baa9f-5a41-4a33-a02f-fcea09e31515",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# MoE configurations\n",
    "MOE_CONFIGS = [\n",
    "    (16, 8),\n",
    "    # (22, 8),\n",
    "    # (8, 8),\n",
    "]\n",
    "\n",
    "params = DistillationParams()\n",
    "path_config = PathConfig()\n",
    "\n",
    "batch_size=4\n",
    "n_batch=2048\n",
    "n_sample=params.batch_size * params.n_batch\n",
    "\n",
    "gradient_accumulation_steps=8\n",
    "\n",
    "calibration = load_dataset(\n",
    "    'cognitivecomputations/dolphin-r1',\n",
    "    \"nonreasoning\",\n",
    "    cache_dir=\"../dolphin-r1\"\n",
    ")\n",
    "\n",
    "calibration = calibration['train']\n",
    "\n",
    "def filter_function(example):\n",
    "    if example['overall_quality'] is not None:\n",
    "        if example['overall_quality'] == 5:\n",
    "            return True\n",
    "    if example['score'] is not None:\n",
    "        if example['score'] >= 0.2:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "\n",
    "calibration = calibration.filter(filter_function)\n",
    "\n",
    "position_ids = torch.arange(\n",
    "    0,\n",
    "    params.max_length,\n",
    "    dtype=torch.long,\n",
    "    device=\"cuda\",\n",
    ").unsqueeze(0)\n",
    "\n",
    "data=calibration['messages'][:n_sample]\n",
    "train_dataset = [tokenizer.apply_chat_template(elt, tokenize=False, add_generation_prompt=False) for elt in tqdm(data)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd12cd68-4603-48f0-b01f-7798413915af",
   "metadata": {},
   "source": [
    "## Distill layers one by one"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a78bb18-46f2-453a-b4cc-bf7507b25978",
   "metadata": {},
   "source": [
    "Now the hard work, loading layers one by one to the GPUs and distill them.\n",
    "\n",
    "Note that a full layer in fp8 should take about 14 go vram during inference with batch size 2.\n",
    "The distiller makes a repartition of the pruned experts on remaining space. With the default config of this notebook, the first distillat is on cuda:0 and the other are in cuda 1.\n",
    "\n",
    "To make it work properly I had to make a custom implementation of FP8Linear layer, optimized for numerical stability, as the default bnb one was producing Nan outputs very frequently. The current implementation is a bit naive, and could probably be improved with custom triton kernels.\n",
    "\n",
    "The new layer was required as i am running the pipeline on Ampere GPU, and Deepseek released kernels can only work with Ada lovelace and plus generations.\n",
    "\n",
    "The logic is hardcoded in the Distiller file, and will need to be updated.\n",
    "\n",
    "With the current config, expect peak total vram usage of about 46 go. The pipeline should take about 24 hour on my setup to run, with a large part dedicated to disk write / read operations and quantization optimisations. If you have a larger vram, you can probably speed it up a bit :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ab8abae-b3a3-4ca1-9d6a-3a8671789455",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Define your configuration and batch size\n",
    "accumulate_steps = 16  # Accumulate gradients over this many batches\n",
    "device = \"cuda\"  # Specify the device\n",
    "\n",
    "# Initialize embedding layer (moved outside the loop)\n",
    "embed_tokens = torch.nn.Embedding(\n",
    "    config.vocab_size,\n",
    "    config.hidden_size,\n",
    "    config.pad_token_id,\n",
    "    device=device\n",
    ")\n",
    "embed_tokens.weight.requires_grad = False\n",
    "embed_tokens.load_state_dict(torch.load(\"layers/embed_tokens.pt\"))\n",
    "embed_tokens.to(device) # move the embedding layer to cuda device\n",
    "\n",
    "# Process each batch\n",
    "all_embeddings = []\n",
    "\n",
    "for batch_idx in tqdm(range(0, params.n_batch, accumulate_steps), desc=\"Processing embeddings\"):\n",
    "    # Collect multiple batches\n",
    "    batches = []\n",
    "    for i in range(accumulate_steps):\n",
    "        current_batch_idx = batch_idx + i\n",
    "        if current_batch_idx >= params.n_batch:\n",
    "            break  # Stop if we reach the end of the dataset\n",
    "\n",
    "        batch_start = current_batch_idx * params.batch_size\n",
    "        batch_end = (current_batch_idx + 1) * params.batch_size\n",
    "        batches.append(train_dataset[batch_start:batch_end])\n",
    "\n",
    "    # Concatenate batches\n",
    "    concatenated_batch = [item for sublist in batches for item in sublist]\n",
    "\n",
    "    # Tokenize the concatenated batch\n",
    "    inputs = tokenizer(\n",
    "        concatenated_batch,\n",
    "        max_length=params.max_length,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        return_tensors='pt'\n",
    "    ).to(device)\n",
    "\n",
    "    # Compute embeddings for accumulated batch\n",
    "    with torch.no_grad():\n",
    "        embeddings = embed_tokens(inputs['input_ids']).to('cpu', dtype=torch.bfloat16)\n",
    "    # Split back into original batch sizes and save\n",
    "    current_idx = 0\n",
    "    for i in range(accumulate_steps):\n",
    "        current_batch_idx = batch_idx + i\n",
    "        if current_batch_idx >= params.n_batch:\n",
    "            break\n",
    "\n",
    "        split_size = params.batch_size\n",
    "        batch_embeddings = embeddings[current_idx * params.max_length : (current_idx + 1) * params.max_length]\n",
    "        \n",
    "        save_intermediate_state(path_config, -1, current_batch_idx, batch_embeddings)\n",
    "\n",
    "        current_idx += 1\n",
    "\n",
    "    if batch_idx % 100 == 0:\n",
    "        memory_cleanup()\n",
    "\n",
    "# Cleanup\n",
    "destruct_module_optimized(embed_tokens)\n",
    "memory_cleanup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81916cb9-d64d-45e1-a5fb-3d1b87da9853",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Initialize tensorboard writer\n",
    "writer = SummaryWriter(path_config.log_dir)\n",
    "# Create distillation configs for each MoE configuration\n",
    "distillation_configs = {\n",
    "    f\"{n_experts}@{n_active}\": DistillationConfig(\n",
    "        adapter_type=\"dora\",\n",
    "        adapter_rank=params.lora_rank,\n",
    "        adapter_alpha=params.lora_alpha,\n",
    "        learning_rate=params.learning_rate,\n",
    "        temperature=params.temperature,\n",
    "        total_steps=params.n_batch,\n",
    "        gradient_accumulation_steps=params.gradient_accumulation_steps\n",
    "    )\n",
    "    for n_experts, n_active in MOE_CONFIGS\n",
    "}\n",
    "\n",
    "# Process each layer\n",
    "for layer_idx in range(61):\n",
    "    # Create and load layer\n",
    "    print(f'Loading layer {layer_idx}')\n",
    "    layer = create_empty_layer_fp8(config, layer_idx=layer_idx)\n",
    "    layer = layer.load_state_dict(torch.load(f'layers/layer_{layer_idx}.pt'), assign=True)\n",
    "    print('layer loaded')\n",
    "    memory_cleanup()\n",
    "\n",
    "\n",
    "    # Store intermediate states\n",
    "    intermediate_states = {}\n",
    "    \n",
    "    if \"DeepseekV3MLP\" in str(layer.mlp.__class__):\n",
    "        # Process standard MLP layer\n",
    "        for batch_idx in tqdm(range(params.n_batch), desc=f\"Processing MLP Layer {layer_idx}\"):\n",
    "            \n",
    "            prev_state = load_intermediate_state(path_config, layer_idx-1, batch_idx)\n",
    "            \n",
    "            with torch.amp.autocast('cuda'):\n",
    "                new_state = layer.forward(\n",
    "                    hidden_states=prev_state.to('cuda:0'),\n",
    "                    position_ids=position_ids,\n",
    "                )[0].detach()\n",
    "                \n",
    "            save_intermediate_state(path_config, layer_idx, batch_idx, new_state)\n",
    "            intermediate_states[batch_idx] = new_state.cpu()\n",
    "            \n",
    "            if batch_idx % 100 == 0:\n",
    "                memory_cleanup()\n",
    "                \n",
    "        destruct_module_optimized(layer)\n",
    "        \n",
    "    else:\n",
    "        # Process MoE layer\n",
    "        distiller = MOEDistillerV3(layer, layer_idx, model_name=path_config.model_name)\n",
    "        \n",
    "        # Prepare calibration data\n",
    "        calibration_batches = [\n",
    "            load_intermediate_state(path_config, layer_idx-1, idx).to('cuda:0', dtype=torch.bfloat16)\n",
    "            for idx in range(min(params.calibration_batches, params.n_batch))\n",
    "        ]\n",
    "        \n",
    "        # Create output directories for each MoE configuration\n",
    "        for n_experts, n_active in MOE_CONFIGS:\n",
    "            os.makedirs(path_config.get_distillation_path(n_experts, n_active), exist_ok=True)\n",
    "        \n",
    "        # Calibrate distiller\n",
    "        distiller.calibrate(\n",
    "            calibration_batches,\n",
    "            position_ids,\n",
    "            MOE_CONFIGS,\n",
    "            distillation_configs[f\"{MOE_CONFIGS[0][0]}@{MOE_CONFIGS[0][1]}\"]  # Use first config as default\n",
    "        )\n",
    "        \n",
    "        memory_cleanup()\n",
    "        \n",
    "        # Training loop\n",
    "        progress_bar = tqdm(range(params.n_batch * params.n_epoch), desc=f\"Training Layer {layer_idx}\")\n",
    "        for batch_idx in progress_bar:\n",
    "            prev_state = load_intermediate_state(\n",
    "                path_config, \n",
    "                layer_idx-1, \n",
    "                batch_idx % params.n_epoch,\n",
    "            )\n",
    "            \n",
    "            new_state, losses = distiller.step(\n",
    "                prev_state.to('cuda:0', dtype=torch.bfloat16),\n",
    "                attention_mask=None,\n",
    "                position_ids=position_ids,\n",
    "            )\n",
    "            \n",
    "            save_intermediate_state(path_config, layer_idx, batch_idx, new_state)\n",
    "            intermediate_states[batch_idx] = new_state.cpu()\n",
    "            \n",
    "            # Log losses\n",
    "            for loss_name, loss_value in losses.items():\n",
    "                writer.add_scalar(\n",
    "                    f\"layer_{layer_idx}/{loss_name}\",\n",
    "                    loss_value,\n",
    "                    batch_idx\n",
    "                )\n",
    "            \n",
    "            progress_bar.set_postfix(**losses)\n",
    "            memory_cleanup()\n",
    "        \n",
    "        # Save and cleanup\n",
    "        distiller.save_distillats()\n",
    "        destruct_module_optimized(layer)\n",
    "        \n",
    "        for distillat in distiller.distillats:\n",
    "            destruct_module_optimized(distillat[\"moe\"])\n",
    "            \n",
    "        memory_cleanup()\n",
    "\n",
    "# Cleanup intermediate files\n",
    "for layer_idx in range(-1, 61):  # Include embedding layer (-1)\n",
    "    for batch_idx in range(params.n_batch):\n",
    "        os.remove(path_config.get_intermediate_path(layer_idx, batch_idx))\n",
    "os.rmdir(path_config.intermediate_dir)\n",
    "\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26ad47a2-5eb4-4737-9991-98481a4127b8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
