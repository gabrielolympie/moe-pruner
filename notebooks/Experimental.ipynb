{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56d7c06a-3a15-45f9-8674-ca79ba0ca52b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "    \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping, Callback, SpikeDetection\n",
    "from pytorch_lightning.loggers import TensorBoardLogger \n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoConfig,\n",
    "    AutoModelForCausalLM,\n",
    "    BitsAndBytesConfig,\n",
    "    get_linear_schedule_with_warmup,\n",
    "    HqqConfig\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model\n",
    "# from ademamix import AdEMAMix\n",
    "import bitsandbytes as bnb\n",
    "\n",
    "from datasets import load_dataset\n",
    "import os\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import gc\n",
    "import argparse\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from torch_utils import memory_cleanup, count_parameters\n",
    "from liger_kernel.transformers import apply_liger_kernel_to_llama\n",
    "\n",
    "from pytorch_lightning.strategies import FSDPStrategy\n",
    "import torch.distributed.fsdp.fully_sharded_data_parallel as fsdp\n",
    "from torch.distributed.fsdp import MixedPrecision\n",
    "\n",
    "from functools import partial\n",
    "from fsdp_utils import fsdp_hqq_dora_model_for_causal_lm, get_wrapping_policy\n",
    "from torch.distributed.fsdp.api import BackwardPrefetch, CPUOffload, ShardingStrategy\n",
    "\n",
    "from torch.distributed.fsdp import FullyShardedDataParallel as FSDP\n",
    "import torch.distributed as dist\n",
    "from model_utils import rsetattr\n",
    "\n",
    "class HealingDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, max_length=512):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.data[idx]\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            max_length=self.max_length,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        return {\n",
    "            \"input_ids\": encoding[\"input_ids\"].squeeze(),\n",
    "            \"attention_mask\": encoding[\"attention_mask\"].squeeze(),\n",
    "        }\n",
    "\n",
    "def load_and_prepare_data(tokenizer, batch_size=8, max_length=512, num_workers=os.cpu_count(), train_sample_limit=None, val_sample_limit=None):\n",
    "    dataset = load_dataset(\n",
    "        \"cognitivecomputations/dolphin-r1\", \"nonreasoning\", cache_dir=\"../dolphin-r1\"\n",
    "    )[\"train\"]\n",
    "\n",
    "    # Apply sample limits if provided\n",
    "    if train_sample_limit is not None:\n",
    "        train_dataset = dataset.select(range(train_sample_limit))  # Use .select for efficiency\n",
    "    else:\n",
    "        train_dataset = dataset\n",
    "\n",
    "    if val_sample_limit is not None:\n",
    "        val_dataset = dataset.select(range(train_sample_limit, train_sample_limit+val_sample_limit)) # Use .select for efficiency\n",
    "    else:\n",
    "        val_dataset = dataset\n",
    "\n",
    "    train_dataset = train_dataset[\"messages\"]\n",
    "    val_dataset = val_dataset[\"messages\"]\n",
    "    \n",
    "    train_dataset = [\n",
    "        tokenizer.apply_chat_template(elt, tokenize=False, add_generation_prompt=False)\n",
    "        for elt in tqdm(train_dataset, desc=\"Preparing dataset train\")\n",
    "    ]\n",
    "\n",
    "    val_dataset = [\n",
    "        tokenizer.apply_chat_template(elt, tokenize=False, add_generation_prompt=False)\n",
    "        for elt in tqdm(val_dataset, desc=\"Preparing dataset train\")\n",
    "    ]\n",
    "\n",
    "    train_dataset = HealingDataset(\n",
    "        train_dataset, tokenizer, max_length=max_length\n",
    "    )\n",
    "    val_dataset = HealingDataset(\n",
    "        val_dataset, tokenizer, max_length=max_length\n",
    "    )\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers, pin_memory=True\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers, pin_memory=True\n",
    "    )\n",
    "\n",
    "    return train_loader, val_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eec67bc-4405-4685-80ac-53c9222cb9bc",
   "metadata": {},
   "source": [
    "## Load Base Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4e19eac-56b7-40d2-94ea-90533abb4bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"0\"\n",
    "os.environ[\"TORCHDYNAMO_DISABLE_GRAPH_CAPTURE\"] = \"1\"\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:128\"\n",
    "\n",
    "torch.set_float32_matmul_precision('medium')\n",
    "# torch.backends.cuda.enable_flash_sdp(True)\n",
    "# torch.backends.cuda.enable_mem_efficient_sdp(True)\n",
    "\n",
    "weights_location=\"deepseek_v3\"\n",
    "n_routed_experts=8\n",
    "n_active_experts=4\n",
    "epochs=1\n",
    "batch_size=1\n",
    "max_length=32\n",
    "\n",
    "learning_rate=3e-5\n",
    "train_sample_limit=32000\n",
    "val_sample_limit=512\n",
    "warmup_steps=128\n",
    "compilation=False\n",
    "checkpoint_every_n_steps=1024\n",
    "accumulate_grad_batches=1\n",
    "\n",
    "\n",
    "model_name=f\"/home/golympie/ai-toolbox/{weights_location}_{n_routed_experts}a{n_active_experts}\" ## i displaced the model on a faster disc for increased loading speed.\n",
    "\n",
    "log_name=f\"{weights_location}_{n_routed_experts}a{n_active_experts}\"\n",
    "log_dir=\"pl_logs\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"deepseek-ai/DeepSeek-V3\", trust_remote_code=True\n",
    ")\n",
    "\n",
    "# Load and prepare data\n",
    "\n",
    "\n",
    "memory_cleanup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c36ecb0a-574b-4f4b-a202-30644d4652a2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "quant_config = BitsAndBytesConfig(\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"fp4\",\n",
    "    bnb_4bit_quant_storage=torch.bfloat16,\n",
    ")\n",
    "\n",
    "\n",
    "import json\n",
    "with open(model_name+\"/model.safetensors.index.json\", \"r\") as f:\n",
    "    weight_map=json.loads(f.read())['weight_map']\n",
    "\n",
    "# quant_config  = HqqConfig(nbits=4, group_size=64)\n",
    "\n",
    "device_map={}\n",
    "for elt in weight_map:\n",
    "    if \"lm_head\" in elt:\n",
    "        device_map[elt]=\"cuda:1\"\n",
    "    elif \"model.embed_tokens\" in elt:\n",
    "        device_map[elt]=\"cuda:0\"\n",
    "    elif \"model.norm\" in elt:\n",
    "        device_map[elt]=\"cuda:0\"\n",
    "    else:\n",
    "        i = int(elt.split('.')[2])\n",
    "        if i < 30:\n",
    "            device_map[elt]=\"cuda:0\"\n",
    "        else:\n",
    "            device_map[elt]=\"cuda:1\"\n",
    "        \n",
    "model=AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=device_map,\n",
    "    attn_implementation=\"flash_attention_2\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    trust_remote_code=True,\n",
    "    offload_buffers=True,\n",
    "    quantization_config=quant_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bce875e9-2cda-403d-b182-aa0ed3ed7f5b",
   "metadata": {},
   "source": [
    "## Adding dora layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afd33290-2d86-4a86-a4ff-a9e588eacff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DORALayer(nn.Module):\n",
    "    \"Same as LORA but also returnes weight norm. This will be wrapped as a single FSDP unit\"\n",
    "    def __init__(self, in_features, out_features, lora_rank, device, dtype, *args, **kwargs):\n",
    "        super().__init__()\n",
    "        # Init LoRA layers.\n",
    "        std_dev = 1 / torch.sqrt(torch.tensor(lora_rank).float()).to(device=device, dtype=torch.bfloat16)\n",
    "        \n",
    "        lora_A_param = nn.Parameter(torch.randn(lora_rank, in_features, device=device, dtype=dtype) * std_dev)\n",
    "        self.lora_A = nn.Linear(in_features, lora_rank, bias=False, device=device, dtype=dtype)\n",
    "        setattr(self.lora_A, \"weight\", lora_A_param)\n",
    "\n",
    "        self.lora_B = nn.Linear(lora_rank, out_features, bias=False, device=device, dtype=dtype)\n",
    "        self.lora_B.weight.data.zero_()\n",
    "\n",
    "    def forward(self, x, frozen_weight):\n",
    "        output = self.lora_B(self.lora_A(x))\n",
    "        column_norm = (frozen_weight + self.lora_B.weight @ self.lora_A.weight).norm(p=2, dim=1).detach()\n",
    "        return output, column_norm\n",
    "\n",
    "class MagnitudeLayer(nn.Module):\n",
    "    \"FSDP doesn't work with nn.ParameterDict hence this module: https://github.com/pytorch/pytorch/issues/79605\"\n",
    "    def __init__(self, vector_data, device, dtype):\n",
    "        super().__init__()\n",
    "        self.magnitude = nn.Parameter(vector_data.to(device=device, dtype=dtype))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x * self.magnitude.view(1, 1, -1)\n",
    "        \n",
    "class BNBDORA(nn.Module):\n",
    "    def __init__(self, base_layer, lora_rank, *args, **kwargs):\n",
    "        super().__init__()\n",
    "        self.base_layer = base_layer\n",
    "        dtype = getattr(base_layer, \"compute_dtype\", next(base_layer.parameters()).dtype)\n",
    "        device = next(base_layer.parameters()).device\n",
    "        \n",
    "        # Init trainable magnitude parameter.\n",
    "        self.magnitude_layer = MagnitudeLayer(self.base_layer.dora_scale.clone().to(dtype=dtype), device, dtype)\n",
    "        self.base_layer.dora_scale = None\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        # Init DORA layers.\n",
    "        self.dora_layer = DORALayer(base_layer.in_features, base_layer.out_features, lora_rank, device, dtype, *args, **kwargs)\n",
    "\n",
    "    def forward(self, x, *args, **kwargs):\n",
    "        result = self.base_layer(x, *args, **kwargs)\n",
    "        result = result.clone()\n",
    "\n",
    "        requires_conversion = not torch.is_autocast_enabled()\n",
    "        if requires_conversion:\n",
    "            expected_dtype = result.dtype\n",
    "            x = x.to(self.dora_layer.lora_A.weight.dtype)\n",
    "\n",
    "        # m * (W + AB / ||W + AB||) @ X == m * ((W @ X + AB @ X) / ||W + AB||)\n",
    "        output, column_norm = self.dora_layer(x, bnb.functional.dequantize_4bit(self.base_layer.weight.data, \n",
    "                                                                                self.base_layer.weight.quant_state))\n",
    "        if requires_conversion:\n",
    "            output = output.to(expected_dtype)\n",
    "        \n",
    "        result += output        \n",
    "        result = result / column_norm.view(1,1,-1) #unit vector result.\n",
    "        result = self.magnitude_layer(result) #rescaled result.\n",
    "        return result\n",
    "\n",
    "class LORA(nn.Module):\n",
    "    def __init__(self, base_layer, lora_rank, lora_alpha, lora_dropout):\n",
    "        super().__init__()\n",
    "        self.base_layer = base_layer\n",
    "        dtype = getattr(base_layer, \"compute_dtype\", next(base_layer.parameters()).dtype)\n",
    "        device = next(base_layer.parameters()).device\n",
    "        lora_A = nn.Linear(base_layer.in_features, lora_rank, bias=False, device=device, dtype=dtype)\n",
    "        lora_B = nn.Linear(lora_rank, base_layer.out_features, bias=False, device=device, dtype=dtype)\n",
    "        lora_B.weight.data.zero_()\n",
    "\n",
    "        self.lora_AB = nn.Sequential(lora_A, lora_B)\n",
    "\n",
    "        self.lora_alpha = lora_alpha\n",
    "        self.lora_dropout = nn.Dropout(lora_dropout)\n",
    "        self.scaling = self.lora_alpha / lora_rank\n",
    "\n",
    "    def forward(self, x: torch.Tensor, *args, **kwargs) -> torch.Tensor:\n",
    "\n",
    "        result = self.base_layer(x, *args, **kwargs)\n",
    "        # As per Tim Dettmers, for 4bit, we need to defensively clone here.\n",
    "        # The reason is that in some cases, an error can occur that backprop\n",
    "        # does not work on a manipulated view. This issue may be solved with\n",
    "        # newer PyTorch versions but this would need extensive testing to be\n",
    "        # sure.\n",
    "        result = result.clone()\n",
    "\n",
    "        requires_conversion = not torch.is_autocast_enabled()\n",
    "        if requires_conversion:\n",
    "            expected_dtype = result.dtype\n",
    "            x = x.to(next(iter(self.lora_AB)).weight.dtype)\n",
    "\n",
    "        output = self.lora_AB(self.lora_dropout(x))\n",
    "        if requires_conversion:\n",
    "            output = output.to(expected_dtype)\n",
    "        output = output * self.scaling\n",
    "\n",
    "        result += output\n",
    "\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fa64251-61c8-4f38-ad38-7a8c1ebe43d1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "target_modules=[]\n",
    "for i in range(n_routed_experts):\n",
    "    target_modules.append(f\"mlp.experts.{i}.gate_proj\")\n",
    "    target_modules.append(f\"mlp.experts.{i}.up_proj\")\n",
    "    target_modules.append(f\"mlp.experts.{i}.down_proj\")\n",
    "\n",
    "# target_modules.append('mlp.gate.weight')\n",
    "\n",
    "lora_rank=16\n",
    "\n",
    "for name, module in tqdm(model.named_modules()):\n",
    "    if isinstance(module, bnb.nn.Linear4bit):\n",
    "        cond=False\n",
    "        for target in target_modules:\n",
    "            if target in name:\n",
    "                cond=True\n",
    "        if cond:\n",
    "            # dora_scale = module.weight.norm(p=2, dim=1).to(dtype=torch.bfloat16)\n",
    "            # rsetattr(\n",
    "            #     model,\n",
    "            #     name+\".dora_scale\",\n",
    "            #     dora_scale\n",
    "            # )\n",
    "            rsetattr(\n",
    "                model,\n",
    "                name,\n",
    "                LORA(module,lora_rank, lora_rank, lora_dropout=0.1)\n",
    "            )\n",
    "\n",
    "\n",
    "for name, params in tqdm(model.named_parameters()):\n",
    "    if any([lora_name in name for lora_name in ['lora_AB', 'lora_A', 'lora_B', 'magnitude', 'mlp.gate.weight']]):\n",
    "        params.requires_grad = True\n",
    "    else:\n",
    "        params.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb7c75a8-685b-4d39-ad88-8e2ba8284e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_parameters(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46cd0f46-3c72-4505-8496-171f33b7fef6",
   "metadata": {},
   "source": [
    "## Splitting layer between cuda 0 and cuda 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e388a7e-5b0b-4a14-a2cc-95591d8bf7e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_prepare_data(tokenizer, batch_size=8, max_length=512, num_workers=os.cpu_count(), train_sample_limit=None, val_sample_limit=None):\n",
    "    dataset = load_dataset(\n",
    "        \"cognitivecomputations/dolphin-r1\", \"nonreasoning\", cache_dir=\"../dolphin-r1\"\n",
    "    )[\"train\"]\n",
    "\n",
    "    def filter_function(example):\n",
    "        if example[\"overall_quality\"] is not None and example[\"overall_quality\"] == 5:\n",
    "            return True\n",
    "        if example[\"score\"] is not None and example[\"score\"] >= 0.2:\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    dataset = dataset.filter(filter_function)\n",
    "    \n",
    "    # Apply sample limits if provided\n",
    "    if train_sample_limit is not None:\n",
    "        train_dataset = dataset.select(range(train_sample_limit))  # Use .select for efficiency\n",
    "    else:\n",
    "        train_dataset = dataset\n",
    "\n",
    "    # if val_sample_limit is not None:\n",
    "    #     val_dataset = dataset.select(range(train_sample_limit, train_sample_limit+val_sample_limit)) # Use .select for efficiency\n",
    "    # else:\n",
    "    #     val_dataset = dataset\n",
    "\n",
    "    train_dataset = train_dataset[\"messages\"]\n",
    "    # val_dataset = val_dataset[\"messages\"]\n",
    "    \n",
    "    train_dataset = [\n",
    "        tokenizer.apply_chat_template(elt, tokenize=False, add_generation_prompt=False)\n",
    "        for elt in tqdm(train_dataset, desc=\"Preparing dataset train\")\n",
    "    ]\n",
    "\n",
    "    # val_dataset = [\n",
    "    #     tokenizer.apply_chat_template(elt, tokenize=False, add_generation_prompt=False)\n",
    "    #     for elt in tqdm(val_dataset, desc=\"Preparing dataset train\")\n",
    "    # ]\n",
    "\n",
    "    train_dataset = HealingDataset(\n",
    "        train_dataset, tokenizer, max_length=max_length\n",
    "    )\n",
    "    # val_dataset = HealingDataset(\n",
    "    #     val_dataset, tokenizer, max_length=max_length\n",
    "    # )\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers, pin_memory=True\n",
    "    )\n",
    "    # val_loader = DataLoader(\n",
    "    #     val_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers, pin_memory=True\n",
    "    # )\n",
    "\n",
    "    return train_loader, None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef8a2f14-9871-45bf-a88a-f39e30f8ac7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from ademamix import AdEMAMix\n",
    "from torch.optim.lr_scheduler import _LRScheduler\n",
    "import math\n",
    "\n",
    "class WarmupCosineAnnealingLR(_LRScheduler):\n",
    "    def __init__(self, optimizer, warmup_steps, total_steps, min_lr=0.0):\n",
    "        self.warmup_steps = warmup_steps\n",
    "        self.total_steps = total_steps\n",
    "        self.min_lr = min_lr\n",
    "        super(WarmupCosineAnnealingLR, self).__init__(optimizer, last_epoch=-1)\n",
    "\n",
    "    def get_lr(self):\n",
    "        if self.last_epoch < self.warmup_steps:\n",
    "            # Linear warmup phase\n",
    "            return [base_lr * (self.last_epoch / self.warmup_steps) for base_lr in self.base_lrs]\n",
    "        else:\n",
    "            # Cosine annealing phase\n",
    "            cosine_decay = 0.5 * (1.0 + math.cos(math.pi * (self.last_epoch - self.warmup_steps) / (self.total_steps - self.warmup_steps)))\n",
    "            decay_factor = (1 - self.min_lr) * cosine_decay + self.min_lr\n",
    "            return [base_lr * decay_factor for base_lr in self.base_lrs]\n",
    "            \n",
    "# Assuming model, tokenizer, and load_and_prepare_data are defined elsewhere\n",
    "\n",
    "total_steps = 32\n",
    "max_length = 128\n",
    "\n",
    "num_epochs=1\n",
    "num_sample = 8192\n",
    "\n",
    "batch_size = 4\n",
    "gradient_accumulation_steps = 1\n",
    "log_interval = gradient_accumulation_steps  # Log every 10 steps\n",
    "\n",
    "lr = 5e-3\n",
    "# Initialize the SummaryWriter\n",
    "writer = SummaryWriter(log_dir='runs/experiment_1')\n",
    "\n",
    "train_loader, val_loader = load_and_prepare_data(\n",
    "    tokenizer, batch_size=batch_size, max_length=max_length,\n",
    "    train_sample_limit=None, val_sample_limit=None\n",
    ")\n",
    "\n",
    "optimizer = AdEMAMix(\n",
    "    filter(lambda p: p.requires_grad, model.parameters()),\n",
    "    lr=lr,\n",
    "    betas=(0.9, 0.999, 0.9999),\n",
    "    alpha=8.0 #batch size is small so increazing alpha to smooth gradient\n",
    ")\n",
    "\n",
    "scheduler = WarmupCosineAnnealingLR(\n",
    "    optimizer,\n",
    "    warmup_steps=128,\n",
    "    total_steps=len(train_loader) // gradient_accumulation_steps,\n",
    "    min_lr=lr/100\n",
    ")\n",
    "\n",
    "\n",
    "model.train()  # Ensure the model is in training mode\n",
    "\n",
    "for epoch in range(num_epochs):  # Assuming num_epochs is defined\n",
    "    for i, encoding in enumerate(tqdm(train_loader)):\n",
    "        input_ids = encoding['input_ids'].to(\"cuda:0\")\n",
    "        attention_mask = encoding['attention_mask'].to(\"cuda:0\")\n",
    "\n",
    "        # Forward pass\n",
    "        output = model(\n",
    "            input_ids=input_ids,\n",
    "            labels=input_ids,  # Assuming labels are the same as input_ids for this task\n",
    "            attention_mask=attention_mask,\n",
    "            use_cache=False,\n",
    "            output_attentions=False,\n",
    "            output_hidden_states=False\n",
    "        )\n",
    "\n",
    "        # Compute loss and backpropagate\n",
    "        loss = output.loss\n",
    "        loss.backward()\n",
    "\n",
    "        # Update model parameters and learning rate\n",
    "        if (i + 1) % gradient_accumulation_steps == 0:\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        # Log loss and learning rate to TensorBoard\n",
    "        if (i + 1) % log_interval == 0:\n",
    "            global_step = epoch * len(train_loader) + i\n",
    "            writer.add_scalar('Loss/train', loss.item(), global_step)\n",
    "            writer.add_scalar('Learning Rate', scheduler.get_last_lr()[0], global_step)\n",
    "\n",
    "# Close the writer\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d884fdd7-a9d3-4734-b34d-c5fe30b51d3e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ce4e1ca-3d85-4a17-9d28-dc23ed7510df",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from transformers import TextStreamer\n",
    "\n",
    "streamer = TextStreamer(tokenizer, skip_prompt=True)\n",
    "\n",
    "model.generation_config.pad_token_id = model.generation_config.eos_token_id\n",
    "\n",
    "messages = [{\"role\": \"user\", \"content\": \"Write a piece of quicksort code in C++\"}]\n",
    "input_tensor = tokenizer.apply_chat_template(messages, add_generation_prompt=True, tokenize=False)\n",
    "input_tensor = tokenizer(input_tensor, return_tensors=\"pt\").to(\"cuda:0\")\n",
    "\n",
    "out = model.generate(**input_tensor, streamer=streamer, temperature=0.01, max_new_tokens=64, do_sample=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36c5d509-b88d-4026-9479-d6ed675e1c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.synchronize(0)\n",
    "torch.cuda.synchronize(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c1678c2-5f6f-474e-a0a2-1394c6d83d20",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
