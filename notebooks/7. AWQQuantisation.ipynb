{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e34002d-92c5-4992-8ff0-19eb0ea4a649",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "780e264d-b6c6-4834-8dba-a15e6ebbaea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "from utils.torch_utils import count_parameters\n",
    "import torch\n",
    "\n",
    "model_path=\"/home/golympie/ai-toolbox/pruned_models/\"+\"deepseek_coder_v2_lite_instruct_fused_2\"\n",
    "\n",
    "model=AutoModelForCausalLM.from_pretrained(\n",
    "    model_path, \n",
    "    device_map='cpu',\n",
    "    trust_remote_code=True\n",
    ")\n",
    "count_parameters(model)\n",
    "\n",
    "# Deepseek-Coder-v2-Lite-Instruct_Fused_2B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cfce656-eb23-477a-bce6-03a59fdcd845",
   "metadata": {},
   "outputs": [],
   "source": [
    "help(model.quantize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5745581d-cb5e-4ee9-805a-1c2110da8815",
   "metadata": {},
   "outputs": [],
   "source": [
    "from awq import AutoAWQForCausalLM\n",
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "\n",
    "model_path = \"/home/golympie/ai-toolbox/pruned_models/Deepseek-Coder-v2-Lite-Instruct_Fused_2B\"\n",
    "quant_path = \"/home/golympie/ai-toolbox/pruned_models/Deepseek-Coder-v2-Lite-Instruct_Fused_2B-AWQ\"\n",
    "\n",
    "quant_config = {\n",
    "    \"zero_point\": True,\n",
    "    \"q_group_size\": 128,\n",
    "    \"w_bit\": 4,\n",
    "    \"version\": \"GEMM\",\n",
    "    # \"max_calib_seq_len\": 256,\n",
    "    # \"n_parallel_calib_samples\":4\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "# Load model\n",
    "model = AutoAWQForCausalLM.from_pretrained(model_path, device_map=\"cuda:1\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)\n",
    "\n",
    "# Quantize\n",
    "model.quantize(\n",
    "    tokenizer, \n",
    "    quant_config=quant_config,\n",
    "    max_calib_seq_len=256,\n",
    "    n_parallel_calib_samples=4\n",
    ")\n",
    "\n",
    "# Save quantized model\n",
    "model.save_quantized(quant_path)\n",
    "tokenizer.save_pretrained(quant_path)\n",
    "\n",
    "print(f'Model is quantized and saved at \"{quant_path}\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f436fea-b775-4b3a-a6e8-60eefa3d1590",
   "metadata": {},
   "outputs": [],
   "source": [
    "export OPENAI_API_BASE=http://localhost:5000/v1/models\n",
    "export OPENAI_API_KEY=c01a4e91d28971ae1a6957d52b55b226\n",
    "\n",
    "lighteval endpoint litellm openai/gpt-4 mmlu \\\n",
    "--max-samples 64 \\\n",
    "--override-batch-size 16\n",
    "\n",
    "\n",
    "lighteval|gpqa\n",
    "lighteval|gsm8k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63412168-251b-47b9-8426-43325475b604",
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepeval.models.base_model import DeepEvalBaseLLM\n",
    "\n",
    "\n",
    "class MirascopeDeepeval(DeepEvalBaseLLM):\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "\n",
    "    def load_model(self):\n",
    "        return self.model\n",
    "\n",
    "    def generate(self, prompt: str) -> str:\n",
    "        chat_model = self.load_model()\n",
    "        return chat_model.invoke(prompt).content\n",
    "\n",
    "    async def a_generate(self, prompt: str) -> str:\n",
    "        chat_model = self.load_model()\n",
    "        res = await chat_model.ainvoke(prompt)\n",
    "        return res.content\n",
    "\n",
    "    def get_model_name(self):\n",
    "        return self.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "571df900-b305-4f0d-a6cc-0f3cf3fbf1f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepeval.benchmarks import MMLU\n",
    "from deepeval.benchmarks.mmlu.task import MMLUTask\n",
    "\n",
    "benchmark = MMLU(tasks=[MMLUTask.HIGH_SCHOOL_COMPUTER_SCIENCE, MMLUTask.ASTRONOMY], n_shots=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1528a78-b0f6-4203-9a56-47fe15a7cc86",
   "metadata": {},
   "outputs": [],
   "source": [
    "help(benchmark.evaluate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6a1eba4-69f5-432f-8619-a5478fa68db0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import AzureChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55b90acf-424a-4c94-8990-a9be4796f7e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import deepeval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf09c663-17f2-4628-a4f5-894ffff25f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir(deepeval.models.DeepEvalBaseLLM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "403c9111-cf27-40f0-94da-4aca4db01954",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
