{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69f9c639-2297-4c3a-92e7-7eddb205b795",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "901972b0-fc72-4e80-8e4b-28bfbe08a011",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoConfig\n",
    "from awq.modules.linear.gemm import WQLinear_GEMM\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from accelerate import init_empty_weights\n",
    "from tqdm.auto import tqdm\n",
    "from copy import deepcopy\n",
    "import shutil\n",
    "import numpy as np\n",
    "import argparse\n",
    "import pickle\n",
    "import torch\n",
    "import json\n",
    "import os\n",
    "\n",
    "from utils.ademamix import AdEMAMix\n",
    "from utils.config_utils import GenerationParams, PathConfig, DistillationParams\n",
    "from utils.experts_merge_utils import dequantize_GEMM\n",
    "from utils.torch_utils import (\n",
    "    destruct_module_optimized,\n",
    "    memory_cleanup,\n",
    "    rsetattr,\n",
    "    load_weights,\n",
    "    rhasattr,\n",
    "    count_parameters\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "551cbb8e-19a6-4140-b77f-09ff78e3ecb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda:1\"\n",
    "\n",
    "model_name = \"../deepseek_coder_v2_lite_instruct_awq\"\n",
    "base_model = \"deepseek-ai/DeepSeek-Coder-V2-Lite-Instruct\"\n",
    "\n",
    "n_epochs = 0\n",
    "start_layer = 0\n",
    "end_layer = 0\n",
    "target_routed_expert = 16\n",
    "target_active_expert = target_routed_expert ## unused in multiplex\n",
    "dora_rank = 8\n",
    "calibrate_merge= True\n",
    "pruning_method= \"fused\"\n",
    "\n",
    "path_config = PathConfig(\n",
    "    model_name = model_name,\n",
    "    intermediate_states = \"../data/intermediate_states\",\n",
    "    expert_states = \"../data/expert_states\",\n",
    "    expert_activations = \"../data/expert_activations\",\n",
    "    distillation_logs = \"distillation_logs\",\n",
    "    moe_states=\"../moe_states\"\n",
    ")\n",
    "\n",
    "distillation_config = DistillationParams(\n",
    "    n_epochs= n_epochs,\n",
    "    target_routed_expert = target_routed_expert,\n",
    "    target_active_expert = target_active_expert,\n",
    "    eval_batches=16,\n",
    "    gradient_accumulation_steps= 4,\n",
    "    learning_rate= 3e-4,\n",
    "    end_factor= 0.2,\n",
    "    calibrate_merge=calibrate_merge,\n",
    "    skip_first_tokens=0, ## useful to avoid tuning on early tokens that have less informations\n",
    "    pruning_method=pruning_method, # topk , act_cl, state_cl\n",
    "    dora_rank=dora_rank,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d259692-4b3d-4819-ac87-32bab2453362",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Loading model')\n",
    "\n",
    "from patched_modules.configuration_deepseek_fused_v2 import DeepseekV2Config\n",
    "from patched_modules.modeling_deepseek_fused_v2 import FusedMOE\n",
    "\n",
    "\n",
    "with open(f\"{model_name}/model.safetensors.index.json\", \"r\") as f:\n",
    "    weight_map = json.load(f)[\"weight_map\"]\n",
    "\n",
    "with init_empty_weights():\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        trust_remote_code=True,\n",
    "        torch_dtype=torch.float16,\n",
    "        attn_implementation=\"flash_attention_2\",\n",
    "        low_cpu_mem_usage=True\n",
    "    )\n",
    "\n",
    "for name, parameter in model.named_parameters():\n",
    "    parameter.requires_grad = False\n",
    "\n",
    "config=AutoConfig.from_pretrained(\n",
    "    base_model,\n",
    "    trust_remote_code=True,\n",
    "    attn_implementation=\"flash_attention_2\",\n",
    "    torch_dtype=torch.float16,\n",
    ")\n",
    "\n",
    "config = config.to_dict()\n",
    "\n",
    "config['auto_map'] = {\n",
    "    'AutoConfig':'configuration_deepseek.DeepseekV2Config',\n",
    "    'AutoModel':'modeling_deepseek.DeepseekV2Model',\n",
    "    'AutoModelForCausalLM':'modeling_deepseek.DeepseekV2ForCausalLM'\n",
    "}\n",
    "\n",
    "config['n_fused_experts']=distillation_config.target_routed_expert\n",
    "config['fused_expert_dora_rank']=distillation_config.dora_rank\n",
    "config['fused_expert_method']=\"mixture\"\n",
    "\n",
    "config=DeepseekV2Config(**config)\n",
    "\n",
    "\n",
    "model.train()\n",
    "destruct_module_optimized(model)\n",
    "memory_cleanup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb378dac-fe0e-42b4-b5ba-8947fe5b9f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf96983e-3d6c-40b8-9f02-66e8f79d5579",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(len(model.model.layers)):\n",
    "    if rhasattr(model, f\"model.layers.{i}.mlp.experts\"):\n",
    "        rsetattr(model, f\"model.layers.{i}.mlp.experts\", torch.nn.Module()) ## ensuring destruction of experts to avoid oom\n",
    "\n",
    "model=model.to_empty(device=\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "342dd298-e04a-4e5d-b759-2f1ff4189bc4",
   "metadata": {},
   "source": [
    "## Load non expert weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7263098-c527-47e6-bc2d-39fa0a03bb5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_modules=[]\n",
    "for elt in weight_map:\n",
    "    if not('.experts.' in elt):\n",
    "        if not('gate.weight' in elt):\n",
    "            target_modules.append(elt)\n",
    "\n",
    "model=load_weights(model, model_name, weight_map, target_modules, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "093aa3a8-6f7f-4896-848d-f543f3b50a46",
   "metadata": {},
   "source": [
    "## Reinitialize experts with new number of layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6642ade7-2127-4481-b703-ee91df6a8df0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer_idx, layer in enumerate(tqdm(model.model.layers)):\n",
    "    if rhasattr(layer.mlp, \"experts\"):\n",
    "        shared=deepcopy(layer.mlp.shared_experts)\n",
    "        \n",
    "        export_path=path_config.moe_states+f\"/distillat_fused_{distillation_config.target_routed_expert}/layer_{layer_idx}\"\n",
    "        state_dict = torch.load(export_path)\n",
    "        \n",
    "        new_state_dict = {}\n",
    "        for key in state_dict.keys():\n",
    "            new_key = key.replace('fused_experts', 'experts')\n",
    "            new_key = new_key.replace('_orig_mod.', '')\n",
    "            \n",
    "            new_state_dict[new_key] = state_dict[key]\n",
    "        \n",
    "        layer.mlp=FusedMOE(config)\n",
    "        \n",
    "        layer.mlp.shared_experts=deepcopy(shared)\n",
    "        layer.mlp.load_state_dict(new_state_dict)\n",
    "        layer.mlp.shared_experts=shared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84b86a5c-b18a-4ed3-b72c-dcf48c9c02b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_parameters(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "001d69f0-0e41-4662-b6c3-24ff997e7b30",
   "metadata": {},
   "source": [
    "## Dequant every WQLinear_GEMM layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "670e8b40-f16d-445e-8415-5948fffbf3fd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model, params = dequantize_GEMM(model, dtype=torch.bfloat16)\n",
    "model.to(device, dtype=torch.bfloat16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffcfffe9-51c7-4e7c-aa4c-71e59adc8c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28e00f6c-451f-4685-a973-48ce04ec6469",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('updating config')\n",
    "\n",
    "model.config=config\n",
    "\n",
    "print('Saving')\n",
    "unhealed_name=model_name+f\"_fused_{distillation_config.target_routed_expert}_unhealed\"\n",
    "unhealed_name=unhealed_name.replace('_awq', '')\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "\n",
    "tokenizer.save_pretrained(unhealed_name)\n",
    "model.save_pretrained(unhealed_name)\n",
    "\n",
    "shutil.copy(os.path.join('../patched_modules/', 'modeling_deepseek_fused_v2.py'), os.path.join(unhealed_name, 'modeling_deepseek.py'))\n",
    "shutil.copy(os.path.join('../patched_modules/', 'configuration_deepseek_fused_v2.py'), os.path.join(unhealed_name, 'configuration_deepseek.py'))\n",
    "\n",
    "shutil.copy(os.path.join('../patched_modules/', 'modeling_deepseek_fused_v2.py'), os.path.join(unhealed_name, 'modeling_deepseek_fused_v2.py'))\n",
    "shutil.copy(os.path.join('../patched_modules/', 'configuration_deepseek_fused_v2.py'), os.path.join(unhealed_name, 'configuration_deepseek_fused_v2.py'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04847743-9536-4afd-995b-fb445ed466b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c42619b4-d945-4346-80aa-48041e2ec8ae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
