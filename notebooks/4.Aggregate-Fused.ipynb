{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69f9c639-2297-4c3a-92e7-7eddb205b795",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "901972b0-fc72-4e80-8e4b-28bfbe08a011",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoConfig\n",
    "from awq.modules.linear.gemm import WQLinear_GEMM\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from accelerate import init_empty_weights\n",
    "from tqdm.auto import tqdm\n",
    "from copy import deepcopy\n",
    "import shutil\n",
    "import numpy as np\n",
    "import argparse\n",
    "import pickle\n",
    "import torch\n",
    "import json\n",
    "import os\n",
    "\n",
    "from utils.ademamix import AdEMAMix\n",
    "from utils.config_utils import GenerationParams, PathConfig, DistillationParams\n",
    "from utils.torch_utils import (\n",
    "    destruct_module_optimized,\n",
    "    memory_cleanup,\n",
    "    rsetattr,\n",
    "    load_weights,\n",
    "    rhasattr,\n",
    "    count_parameters\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "551cbb8e-19a6-4140-b77f-09ff78e3ecb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cpu\"\n",
    "\n",
    "model_name = \"../deepseek_v3_awq\"\n",
    "base_model = \"unsloth/DeepSeek-V3-bf16\"\n",
    "\n",
    "# model_name = \"../deepseek_v2_lite_chat_awq\"\n",
    "# base_model = \"deepseek-ai/DeepSeek-V2-Lite-Chat\"\n",
    "\n",
    "n_epochs = 0\n",
    "start_layer = 0\n",
    "end_layer = 0\n",
    "target_routed_expert = 4\n",
    "target_active_expert = target_routed_expert ## unused in multiplex\n",
    "dora_rank = 4\n",
    "calibrate_merge= True\n",
    "pruning_method= \"fused\"\n",
    "\n",
    "path_config = PathConfig(\n",
    "    model_name = model_name,\n",
    "    intermediate_states = \"../data/intermediate_states\",\n",
    "    expert_states = \"../data/expert_states\",\n",
    "    expert_activations = \"../data/expert_activations\",\n",
    "    distillation_logs = \"distillation_logs\",\n",
    "    moe_states=\"../moe_states\"\n",
    ")\n",
    "\n",
    "distillation_config = DistillationParams(\n",
    "    n_epochs= n_epochs,\n",
    "    target_routed_expert = target_routed_expert,\n",
    "    target_active_expert = target_active_expert,\n",
    "    eval_batches=16,\n",
    "    gradient_accumulation_steps= 4,\n",
    "    learning_rate= 3e-4,\n",
    "    end_factor= 0.2,\n",
    "    calibrate_merge=calibrate_merge,\n",
    "    skip_first_tokens=0, ## useful to avoid tuning on early tokens that have less informations\n",
    "    pruning_method=pruning_method, # topk , act_cl, state_cl\n",
    "    dora_rank=dora_rank,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d259692-4b3d-4819-ac87-32bab2453362",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Loading model')\n",
    "\n",
    "from patched_modules.configuration_deepseek import DeepseekV3Config\n",
    "from patched_modules.modeling_deepseek_fused import FusedMOE\n",
    "\n",
    "# from patched_modules.configuration_deepseek_fused_v2 import DeepseekV2Config as DeepseekV3Config\n",
    "# from patched_modules.modeling_deepseek_fused_v2 import FusedMOE\n",
    "\n",
    "from utils.torch_utils import convert_meta_model_to_awq\n",
    "\n",
    "\n",
    "config=AutoConfig.from_pretrained(\n",
    "    model_name,\n",
    "    trust_remote_code=True,\n",
    "    attn_implementation=\"flash_attention_2\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    low_cpu_mem_usage=True\n",
    ")\n",
    "\n",
    "with open(f\"{model_name}/model.safetensors.index.json\", \"r\") as f:\n",
    "    weight_map = json.load(f)[\"weight_map\"]\n",
    "\n",
    "with init_empty_weights():\n",
    "    # model = AutoModelForCausalLM.from_pretrained(\n",
    "    #     model_name,\n",
    "    #     trust_remote_code=True,\n",
    "    #     torch_dtype=torch.bfloat16,\n",
    "    #     attn_implementation=\"flash_attention_2\",\n",
    "    #     low_cpu_mem_usage=True\n",
    "    # )\n",
    "    model = AutoModelForCausalLM.from_config(\n",
    "        config,\n",
    "        trust_remote_code=True,\n",
    "        # torch_dtype=dtype,\n",
    "        # attn_implementation=\"flash_attention_2\",\n",
    "        # low_cpu_mem_usage=True\n",
    "    )\n",
    "\n",
    "\n",
    "model=convert_meta_model_to_awq(model, config, device)\n",
    "\n",
    "for name, parameter in model.named_parameters():\n",
    "    parameter.requires_grad = False\n",
    "\n",
    "config=AutoConfig.from_pretrained(\n",
    "    '../patched_modules/',\n",
    "    trust_remote_code=True,\n",
    "    attn_implementation=\"flash_attention_2\",\n",
    "    torch_dtype=torch.float16,\n",
    ")\n",
    "\n",
    "# config=AutoConfig.from_pretrained(\n",
    "#     base_model,\n",
    "#     trust_remote_code=True,\n",
    "#     attn_implementation=\"flash_attention_2\",\n",
    "#     torch_dtype=torch.float16,\n",
    "# )\n",
    "\n",
    "config = config.to_dict()\n",
    "\n",
    "config['auto_map'] = {\n",
    "    'AutoConfig':'configuration_deepseek.DeepseekV3Config',\n",
    "    'AutoModel':'modeling_deepseek.DeepseekV3Model',\n",
    "    'AutoModelForCausalLM':'modeling_deepseek.DeepseekV3ForCausalLM'\n",
    "}\n",
    "\n",
    "# config['auto_map'] = {\n",
    "#     'AutoConfig':'configuration_deepseek.DeepseekV2Config',\n",
    "#     'AutoModel':'modeling_deepseek.DeepseekV2Model',\n",
    "#     'AutoModelForCausalLM':'modeling_deepseek.DeepseekV2ForCausalLM'\n",
    "# }\n",
    "\n",
    "config['n_fused_experts']=distillation_config.target_routed_expert\n",
    "config['fused_expert_dora_rank']=distillation_config.dora_rank\n",
    "config['fused_expert_method']=\"mixture\"\n",
    "\n",
    "config=DeepseekV3Config(**config)\n",
    "\n",
    "\n",
    "model.train()\n",
    "destruct_module_optimized(model)\n",
    "memory_cleanup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf96983e-3d6c-40b8-9f02-66e8f79d5579",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(len(model.model.layers)):\n",
    "    if rhasattr(model, f\"model.layers.{i}.mlp.experts\"):\n",
    "        rsetattr(model, f\"model.layers.{i}.mlp.experts\", torch.nn.Module()) ## ensuring destruction of experts to avoid oom\n",
    "\n",
    "model=model.to_empty(device=\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "342dd298-e04a-4e5d-b759-2f1ff4189bc4",
   "metadata": {},
   "source": [
    "## Load non expert weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7263098-c527-47e6-bc2d-39fa0a03bb5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_modules=[]\n",
    "for elt in weight_map:\n",
    "    if not('.experts.' in elt):\n",
    "        if not('gate.weight' in elt):\n",
    "            target_modules.append(elt)\n",
    "\n",
    "model=load_weights(model, model_name, weight_map, target_modules, \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "093aa3a8-6f7f-4896-848d-f543f3b50a46",
   "metadata": {},
   "source": [
    "## Reinitialize experts with new number of layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6642ade7-2127-4481-b703-ee91df6a8df0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "for layer_idx, layer in enumerate(tqdm(model.model.layers)):\n",
    "    if rhasattr(layer.mlp, \"experts\"):\n",
    "        shared=deepcopy(layer.mlp.shared_experts)\n",
    "        \n",
    "        export_path=path_config.moe_states+f\"/distillat_fused_{distillation_config.target_routed_expert}/layer_{layer_idx}\"\n",
    "        state_dict = torch.load(export_path)\n",
    "        \n",
    "        new_state_dict = {}\n",
    "        for key in state_dict.keys():\n",
    "            new_key = key.replace('fused_experts', 'experts')\n",
    "            new_key = new_key.replace('_orig_mod.', '')\n",
    "            \n",
    "            new_state_dict[new_key] = state_dict[key]\n",
    "        \n",
    "        layer.mlp=FusedMOE(config)\n",
    "        \n",
    "        layer.mlp.shared_experts=deepcopy(shared)\n",
    "        layer.mlp.load_state_dict(new_state_dict)\n",
    "        layer.mlp.shared_experts=shared\n",
    "\n",
    "        gc.collect()\n",
    "\n",
    "        del shared\n",
    "        memory_cleanup()\n",
    "\n",
    "# model=model.to(device, dtype=torch.bfloat16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f3167ec-45a7-4d56-899e-7e93c308788e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=model.to(device)\n",
    "memory_cleanup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84b86a5c-b18a-4ed3-b72c-dcf48c9c02b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_parameters(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "001d69f0-0e41-4662-b6c3-24ff997e7b30",
   "metadata": {},
   "source": [
    "## Dequant every WQLinear_GEMM layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "670e8b40-f16d-445e-8415-5948fffbf3fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from awq.modules.linear.gemm import WQLinear_GEMM\n",
    "from awq.modules.triton.gemm import awq_gemm_triton, awq_dequantize_triton\n",
    "\n",
    "def dequantize_WQLinear_GEMM(wq_linear, destruct=True, dtype=torch.bfloat16, device='cpu'):\n",
    "    quant_params={\n",
    "        'w_bit': wq_linear.w_bit,\n",
    "        'group_size': wq_linear.group_size,\n",
    "        'in_features': wq_linear.in_features,\n",
    "        'out_features': wq_linear.out_features,\n",
    "        'bias': wq_linear.bias is not None,\n",
    "        'dev':wq_linear.qweight.device,\n",
    "        'zero_point':wq_linear.qzeros is not None,\n",
    "    }\n",
    "    \n",
    "    linear=torch.nn.Linear(wq_linear.in_features, wq_linear.out_features, bias=wq_linear.bias is not None, device=wq_linear.qweight.device, dtype=dtype)\n",
    "\n",
    "    wq_linear=wq_linear.to('cuda:0')\n",
    "    linear.weight=torch.nn.Parameter(\n",
    "        awq_dequantize_triton(\n",
    "            wq_linear.qweight,\n",
    "            wq_linear.scales,\n",
    "            wq_linear.qzeros,\n",
    "        ).T,\n",
    "        requires_grad=True\n",
    "    )\n",
    "    \n",
    "    if wq_linear.bias is not None:\n",
    "        linear.bias = torch.nn.Parameter(wq_linear.bias, requires_grad=True)\n",
    "    \n",
    "    if destruct:\n",
    "        wq_linear.to_empty(device=\"meta\")\n",
    "        del wq_linear\n",
    "    return linear.to(dtype=dtype, device=device)\n",
    "\n",
    "count=0\n",
    "for name, module in tqdm(model.named_modules()):\n",
    "    if isinstance(module, WQLinear_GEMM):\n",
    "        rsetattr(model, name, dequantize_WQLinear_GEMM(module, destruct=True, dtype=torch.bfloat16, device='cpu'))\n",
    "        count+=1\n",
    "        if count==100:\n",
    "            count=0\n",
    "            memory_cleanup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffcfffe9-51c7-4e7c-aa4c-71e59adc8c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d83e8fdb-7f6b-4048-88b2-5513d7c8d48a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.lm_head.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28e00f6c-451f-4685-a973-48ce04ec6469",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('updating config')\n",
    "\n",
    "model.config=config\n",
    "\n",
    "print('Saving')\n",
    "unhealed_name=model_name+f\"_fused_{distillation_config.target_routed_expert}_unhealed\"\n",
    "unhealed_name=unhealed_name.replace('_awq', '').replace(\"../\",\"/home/golympie/\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "\n",
    "tokenizer.save_pretrained(unhealed_name)\n",
    "model.save_pretrained(unhealed_name)\n",
    "\n",
    "shutil.copy(os.path.join('../patched_modules/', 'modeling_deepseek_fused.py'), os.path.join(unhealed_name, 'modeling_deepseek.py'))\n",
    "shutil.copy(os.path.join('../patched_modules/', 'configuration_deepseek.py'), os.path.join(unhealed_name, 'configuration_deepseek.py'))\n",
    "\n",
    "# shutil.copy(os.path.join('../patched_modules/', 'modeling_deepseek_fused_v2.py'), os.path.join(unhealed_name, 'modeling_deepseek.py'))\n",
    "# shutil.copy(os.path.join('../patched_modules/', 'configuration_deepseek_fused_v2.py'), os.path.join(unhealed_name, 'configuration_deepseek.py'))\n",
    "\n",
    "# shutil.copy(os.path.join('../patched_modules/', 'modeling_deepseek_fused_v2.py'), os.path.join(unhealed_name, 'modeling_deepseek_fused_v2.py'))\n",
    "# shutil.copy(os.path.join('../patched_modules/', 'configuration_deepseek_fused_v2.py'), os.path.join(unhealed_name, 'configuration_deepseek_fused_v2.py'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c42619b4-d945-4346-80aa-48041e2ec8ae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
