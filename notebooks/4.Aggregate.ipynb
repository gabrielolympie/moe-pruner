{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69f9c639-2297-4c3a-92e7-7eddb205b795",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "901972b0-fc72-4e80-8e4b-28bfbe08a011",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoConfig\n",
    "from awq.modules.linear.gemm import WQLinear_GEMM\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from accelerate import init_empty_weights\n",
    "from tqdm.auto import tqdm\n",
    "from copy import deepcopy\n",
    "import numpy as np\n",
    "import shutil\n",
    "import pickle\n",
    "import torch\n",
    "import json\n",
    "import os\n",
    "\n",
    "from utils.ademamix import AdEMAMix\n",
    "from utils.config_utils import GenerationParams, PathConfig, DistillationParams\n",
    "from utils.expert_merge_utils import calibrated_merge_experts, dequantize_GEMM\n",
    "from utils.experts_gate_utils import create_gate\n",
    "from utils.torch_utils import (\n",
    "    save_quant,\n",
    "    load_quant,\n",
    "    destruct_module_optimized,\n",
    "    memory_cleanup,\n",
    "    get_nonreasoning_dataset,\n",
    "    load_weight,\n",
    "    rsetattr,\n",
    "    rgetattr,\n",
    "    load_weights,\n",
    "    rhasattr,\n",
    "    count_parameters\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "551cbb8e-19a6-4140-b77f-09ff78e3ecb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"../deepseek_v2_lite_awq\"\n",
    "device=\"cuda:0\"\n",
    "\n",
    "path_config = PathConfig(\n",
    "    model_name = model_name,\n",
    "    intermediate_states = \"../data/intermediate_states\",\n",
    "    expert_states = \"../data/expert_states\",\n",
    "    expert_activations = \"../data/expert_activations\",\n",
    "    distillation_logs = \"../distillation_logs\",\n",
    "    moe_states=\"../moe_states\"\n",
    ")\n",
    "\n",
    "distillation_config = DistillationParams(\n",
    "    n_epochs= 10,\n",
    "    target_routed_expert = 4,\n",
    "    target_active_expert = 2,\n",
    "    eval_batches=16,\n",
    "    gradient_accumulation_steps= 1,\n",
    "    learning_rate= 6e-4,\n",
    "    end_factor= 0.05,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d259692-4b3d-4819-ac87-32bab2453362",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"{model_name}/model.safetensors.index.json\", \"r\") as f:\n",
    "    weight_map = json.load(f)[\"weight_map\"]\n",
    "\n",
    "config=AutoConfig.from_pretrained(\n",
    "    model_name,\n",
    "    trust_remote_code=True,\n",
    "    attn_implementation=\"flash_attention_2\",\n",
    "    torch_dtype=torch.float16,\n",
    ")\n",
    "\n",
    "config.n_routed_experts=distillation_config.target_routed_expert\n",
    "config.num_experts_per_tok=distillation_config.target_active_expert\n",
    "\n",
    "with init_empty_weights():\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        trust_remote_code=True,\n",
    "        torch_dtype=torch.float16,\n",
    "        attn_implementation=\"flash_attention_2\",\n",
    "        low_cpu_mem_usage=True\n",
    "    )\n",
    "\n",
    "for name, parameter in model.named_parameters():\n",
    "    parameter.requires_grad = False\n",
    "\n",
    "\n",
    "model.train()\n",
    "destruct_module_optimized(model)\n",
    "memory_cleanup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf96983e-3d6c-40b8-9f02-66e8f79d5579",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(len(model.model.layers)):\n",
    "    if rhasattr(model, f\"model.layers.{i}.mlp.experts\"):\n",
    "        rsetattr(model, f\"model.layers.{i}.mlp.experts\", torch.nn.Module()) ## ensuring destruction of experts to avoid oom\n",
    "\n",
    "model=model.to_empty(device=\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "342dd298-e04a-4e5d-b759-2f1ff4189bc4",
   "metadata": {},
   "source": [
    "## Load non expert weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7263098-c527-47e6-bc2d-39fa0a03bb5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_modules=[]\n",
    "for elt in weight_map:\n",
    "    if not('.experts.' in elt):\n",
    "        if not('gate.weight' in elt):\n",
    "            target_modules.append(elt)\n",
    "\n",
    "model=load_weights(model, model_name, weight_map, target_modules, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "093aa3a8-6f7f-4896-848d-f543f3b50a46",
   "metadata": {},
   "source": [
    "## Reinitialize experts with new number of layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6642ade7-2127-4481-b703-ee91df6a8df0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for layer_idx, layer in enumerate(tqdm(model.model.layers)):\n",
    "    if rhasattr(layer.mlp, \"experts\"):\n",
    "        shared=deepcopy(layer.mlp.shared_experts)\n",
    "        layer.mlp.__init__(config)\n",
    "        layer.mlp.shared_experts=shared\n",
    "        \n",
    "        export_path=path_config.moe_states+f\"/distillat_{distillation_config.target_routed_expert}a{distillation_config.target_active_expert}/layer_{layer_idx}\"\n",
    "        layer.mlp.load_state_dict(torch.load(export_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84b86a5c-b18a-4ed3-b72c-dcf48c9c02b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_parameters(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "001d69f0-0e41-4662-b6c3-24ff997e7b30",
   "metadata": {},
   "source": [
    "## Dequant every WQLinear_GEMM layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "670e8b40-f16d-445e-8415-5948fffbf3fd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model, params = dequantize_GEMM(model, dtype=torch.bfloat16)\n",
    "model.to('cuda:0', dtype=torch.bfloat16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28e00f6c-451f-4685-a973-48ce04ec6469",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('updating config')\n",
    "config=AutoConfig.from_pretrained(\n",
    "    base_model,\n",
    "    attn_implementation=\"flash_attention_2\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "config.n_routed_experts=distillation_config.target_routed_expert\n",
    "config.num_experts_per_tok=distillation_config.target_active_expert\n",
    "\n",
    "model.config=config\n",
    "\n",
    "print('Saving')\n",
    "unhealed_name=model_name+f\"_{distillation_config.target_routed_expert}a{distillation_config.target_active_expert}_unhealed\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "\n",
    "tokenizer.save_pretrained(unhealed_name)\n",
    "model.save_pretrained(unhealed_name)\n",
    "\n",
    "shutil.copy(os.path.join(model_name, 'modeling_deepseek.py'), os.path.join(unhealed_name, 'modeling_deepseek.py'))\n",
    "shutil.copy(os.path.join(model_name, 'configuration_deepseek.py'), os.path.join(unhealed_name, 'configuration_deepseek.py'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04847743-9536-4afd-995b-fb445ed466b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c42619b4-d945-4346-80aa-48041e2ec8ae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
