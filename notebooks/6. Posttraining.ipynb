{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56277875-f169-489f-8ae0-d704c82f66f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "from bitsandbytes.optim.ademamix import AdEMAMix\n",
    "\n",
    "# Define a simple model to compare optimizer memory footprints\n",
    "model = torch.nn.Linear(10, 1)\n",
    "\n",
    "# Initialize optimizers\n",
    "adamw = optim.AdamW(model.parameters())\n",
    "sgd = optim.SGD(model.parameters())\n",
    "adam = optim.Adam(model.parameters())\n",
    "ademamix = AdEMAMix(model.parameters())\n",
    "\n",
    "\n",
    "# Function to calculate memory footprint\n",
    "def calculate_memory_footprint(optimizer):\n",
    "    total_memory = 0\n",
    "    for param in model.parameters():\n",
    "        total_memory += param.element_size() * param.nelement()\n",
    "        if param in optimizer.state:\n",
    "            for value in optimizer.state[param].values():\n",
    "                if isinstance(value, torch.Tensor):\n",
    "                    total_memory += value.element_size() * value.nelement()\n",
    "    return total_memory\n",
    "\n",
    "\n",
    "# Calculate memory footprints\n",
    "adamw_memory = calculate_memory_footprint(adamw)\n",
    "sgd_memory = calculate_memory_footprint(sgd)\n",
    "adam_memory = calculate_memory_footprint(adam)\n",
    "ademamix_memory = calculate_memory_footprint(ademamix)\n",
    "\n",
    "# Calculate relative memory footprints\n",
    "relative_memory_footprints = {\n",
    "    \"AdamW\": adamw_memory,\n",
    "    \"SGD\": sgd_memory,\n",
    "    \"Adam\": adam_memory,\n",
    "    \"AdEMAMix\": ademamix_memory,\n",
    "}\n",
    "\n",
    "print(relative_memory_footprints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7376cecf-af1e-4c48-8803-0e84e003e4fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import (\n",
    "    ModelCheckpoint,\n",
    "    EarlyStopping,\n",
    "    Callback,\n",
    "    SpikeDetection,\n",
    ")\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoConfig,\n",
    "    AutoModelForCausalLM,\n",
    "    BitsAndBytesConfig,\n",
    "    get_linear_schedule_with_warmup,\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from ademamix import AdEMAMix\n",
    "from datasets import load_dataset\n",
    "import os\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import gc\n",
    "import argparse\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from torch_utils import memory_cleanup, count_parameters\n",
    "from liger_kernel.transformers import apply_liger_kernel_to_llama\n",
    "\n",
    "apply_liger_kernel_to_llama()\n",
    "\n",
    "torch.set_float32_matmul_precision(\"medium\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc4f2c49-faf9-4af4-a8d2-1b23b4ecbef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"cognitivecomputations/dolphin-r1\", \"nonreasoning\", cache_dir=\"../dolphin-r1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0546db3c-42ca-4bfd-9f42-496054501e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HealingDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, max_length=512):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.data[idx]\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            max_length=self.max_length,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        return {\n",
    "            \"input_ids\": encoding[\"input_ids\"].squeeze(),\n",
    "            \"attention_mask\": encoding[\"attention_mask\"].squeeze(),\n",
    "        }\n",
    "\n",
    "\n",
    "def load_and_prepare_data(\n",
    "    tokenizer,\n",
    "    batch_size=8,\n",
    "    max_length=512,\n",
    "    num_workers=os.cpu_count(),\n",
    "    train_sample_limit=None,\n",
    "    val_sample_limit=None,\n",
    "):\n",
    "    dataset = load_dataset(\"cognitivecomputations/dolphin-r1\", \"nonreasoning\", cache_dir=\"../dolphin-r1\")[\"train\"]\n",
    "\n",
    "    # Apply sample limits if provided\n",
    "    if train_sample_limit is not None:\n",
    "        train_dataset = dataset.select(range(train_sample_limit))  # Use .select for efficiency\n",
    "    else:\n",
    "        train_dataset = dataset\n",
    "\n",
    "    if val_sample_limit is not None:\n",
    "        val_dataset = dataset.select(\n",
    "            range(train_sample_limit, train_sample_limit + val_sample_limit)\n",
    "        )  # Use .select for efficiency\n",
    "    else:\n",
    "        val_dataset = dataset\n",
    "\n",
    "    train_dataset = train_dataset[\"messages\"]\n",
    "    val_dataset = val_dataset[\"messages\"]\n",
    "\n",
    "    train_dataset = [\n",
    "        tokenizer.apply_chat_template(elt, tokenize=False, add_generation_prompt=False)\n",
    "        for elt in tqdm(train_dataset, desc=\"Preparing dataset train\")\n",
    "    ]\n",
    "\n",
    "    val_dataset = [\n",
    "        tokenizer.apply_chat_template(elt, tokenize=False, add_generation_prompt=False)\n",
    "        for elt in tqdm(val_dataset, desc=\"Preparing dataset train\")\n",
    "    ]\n",
    "\n",
    "    train_dataset = HealingDataset(train_dataset, tokenizer, max_length=max_length)\n",
    "    val_dataset = HealingDataset(val_dataset, tokenizer, max_length=max_length)\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "\n",
    "    return train_loader, val_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f39a29de-63fd-477b-83d8-baf89e3a153c",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_location = \"deepseek_v3\"\n",
    "n_routed_experts = 8\n",
    "n_active_experts = 4\n",
    "\n",
    "model_name = f\"/home/golympie/ai-toolbox/{weights_location}_{n_routed_experts}a{n_active_experts}\"  ## i displaced the model on a faster disc for increased loading speed.\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    # load_in_8bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"fp4\",\n",
    "    bnb_4bit_quant_storage=torch.bfloat16,\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    trust_remote_code=True,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    attn_implementation=\"flash_attention_2\",\n",
    "    device_map=\"auto\",\n",
    "    quantization_config=bnb_config,\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"deepseek-ai/DeepSeek-V3\", trust_remote_code=True)\n",
    "\n",
    "target_modules = []\n",
    "for i in range(n_routed_experts):\n",
    "    target_modules.append(f\"mlp.experts.{i}.gate_proj\")\n",
    "    target_modules.append(f\"mlp.experts.{i}.up_proj\")\n",
    "    target_modules.append(f\"mlp.experts.{i}.down_proj\")\n",
    "\n",
    "target_modules.append(\"mlp.gate.weight\")  ## Add all gate at once\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    use_dora=True,\n",
    "    r=8,  # Rank of the low-rank matrices\n",
    "    lora_alpha=8,  # Scaling factor\n",
    "    target_modules=target_modules,\n",
    "    lora_dropout=0.1,  # Dropout rate\n",
    "    bias=\"none\",  # Whether to add bias\n",
    "    task_type=\"CAUSAL_LM\",  # Task type (Causal Language Modeling),\n",
    ")\n",
    "\n",
    "# Apply LoRA to the model\n",
    "model = get_peft_model(model, lora_config)\n",
    "count_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46692b1f-b100-4e9e-8462-0bd8c0b0cc4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LightningModel(pl.LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model,\n",
    "        tokenizer,\n",
    "        learning_rate=2e-5,\n",
    "        warmup_steps=100,\n",
    "        total_steps=1000,\n",
    "        min_lr=1e-6,\n",
    "        compilation=True,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.config_dict = model.config.to_dict()\n",
    "\n",
    "        self.model = model\n",
    "\n",
    "        if compilation:\n",
    "            print(\"compile model\")\n",
    "            self.model = torch.compile(self.model)\n",
    "\n",
    "        self.tokenizer = tokenizer\n",
    "        self.learning_rate = learning_rate\n",
    "        self.warmup_steps = warmup_steps\n",
    "        self.total_steps = total_steps\n",
    "        self.min_lr = min_lr  # Minimum learning rate for cosine annealing\n",
    "\n",
    "        # Initialize EMA tracking\n",
    "        self.loss_ema = None\n",
    "        self.ema_alpha = 2.0 / (10 + 1)  # Alpha for 10-step EMA\n",
    "\n",
    "        self.save_hyperparameters(ignore=[\"tokenizer\", \"config\"])\n",
    "        self.hparams.update({\"model_config\": self.config_dict})\n",
    "        memory_cleanup()\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None, labels=None):\n",
    "        return self.model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        outputs = self(\n",
    "            batch[\"input_ids\"],\n",
    "            attention_mask=batch[\"attention_mask\"],\n",
    "            labels=batch[\"input_ids\"],\n",
    "        )\n",
    "        loss = outputs.loss\n",
    "\n",
    "        # Update EMA\n",
    "        if self.loss_ema is None:\n",
    "            self.loss_ema = loss.item()\n",
    "        else:\n",
    "            self.loss_ema = (1 - self.ema_alpha) * self.loss_ema + self.ema_alpha * loss.item()\n",
    "\n",
    "        # Replace loss with EMA if it's more than 2.5x the EMA\n",
    "        original_loss = loss.item()\n",
    "        if original_loss > 1.5 * self.loss_ema:\n",
    "            # Create a new tensor with the EMA value, maintaining gradients\n",
    "            loss = loss * (self.loss_ema / original_loss)\n",
    "            self.log(\"loss_clipped\", True, on_step=True, prog_bar=True, sync_dist=True)\n",
    "        else:\n",
    "            self.log(\"loss_clipped\", False, on_step=True, prog_bar=True, sync_dist=True)\n",
    "\n",
    "        # Log losses\n",
    "        self.log(\n",
    "            \"train_loss\",\n",
    "            loss,\n",
    "            on_step=True,\n",
    "            on_epoch=True,\n",
    "            prog_bar=True,\n",
    "            sync_dist=True,\n",
    "        )\n",
    "        self.log(\n",
    "            \"train_loss_ema\",\n",
    "            self.loss_ema,\n",
    "            on_step=True,\n",
    "            on_epoch=True,\n",
    "            prog_bar=True,\n",
    "            sync_dist=True,\n",
    "        )\n",
    "        self.log(\n",
    "            \"train_loss_original\",\n",
    "            original_loss,\n",
    "            on_step=True,\n",
    "            prog_bar=True,\n",
    "            sync_dist=True,\n",
    "        )\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        outputs = self(\n",
    "            batch[\"input_ids\"],\n",
    "            attention_mask=batch[\"attention_mask\"],\n",
    "            labels=batch[\"input_ids\"],\n",
    "        )\n",
    "        loss = outputs.loss\n",
    "        self.log(\"val_loss\", loss, on_epoch=True, prog_bar=True, sync_dist=True)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = AdEMAMix(self.parameters(), lr=self.learning_rate)\n",
    "        # Use CosineAnnealingLR scheduler\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "            optimizer,\n",
    "            T_max=self.total_steps,  # Adjusted total steps\n",
    "            eta_min=self.min_lr,  # Minimum learning rate\n",
    "        )\n",
    "\n",
    "        if self.warmup_steps > 0:\n",
    "\n",
    "            def lr_lambda(step):\n",
    "                if step < self.warmup_steps:\n",
    "                    return float(step) / float(max(1, self.warmup_steps))\n",
    "                return scheduler.get_lr()[0] / self.learning_rate  # Scale by initial LR\n",
    "\n",
    "            warmup_scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n",
    "\n",
    "            return [optimizer], [\n",
    "                {\"scheduler\": warmup_scheduler, \"interval\": \"step\"},  # Warmup scheduler\n",
    "                {\n",
    "                    \"scheduler\": scheduler,\n",
    "                    \"interval\": \"step\",\n",
    "                    \"start_epoch\": self.warmup_steps,\n",
    "                },\n",
    "            ]\n",
    "\n",
    "        return {\n",
    "            \"optimizer\": optimizer,\n",
    "            \"lr_scheduler\": {\n",
    "                \"scheduler\": scheduler,\n",
    "                \"interval\": \"step\",\n",
    "            },\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "848646a3-0fea-407c-9a5f-4c1cf3ce782c",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 1\n",
    "batch_size = 4\n",
    "max_length = 512\n",
    "\n",
    "learning_rate = 3e-5\n",
    "train_sample_limit = 32000\n",
    "val_sample_limit = 512\n",
    "warmup_steps = 128\n",
    "compilation = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "052581f2-6082-4e35-9e61-e2cc3f0029c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, val_loader = load_and_prepare_data(\n",
    "    tokenizer,\n",
    "    batch_size=batch_size,\n",
    "    max_length=max_length,\n",
    "    train_sample_limit=train_sample_limit,\n",
    "    val_sample_limit=val_sample_limit,\n",
    ")\n",
    "\n",
    "# Calculate total steps for the scheduler\n",
    "total_steps = len(train_loader) * epochs\n",
    "\n",
    "pl_model = LightningModel(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    learning_rate=learning_rate,\n",
    "    warmup_steps=warmup_steps,\n",
    "    total_steps=total_steps,\n",
    "    compilation=compilation,\n",
    ")\n",
    "\n",
    "# Callbacks\n",
    "os.makedirs(\"checkpoints_full/\", exist_ok=True)\n",
    "os.makedirs(\"checkpoints_full/\" + log_name, exist_ok=True)\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    dirpath=\"checkpoints_full/\" + log_name,  # Specify the directory to save checkpoints\n",
    "    filename=\"{epoch}-{step}-{train_loss:.4f}\",  # Define the checkpoint filename format\n",
    "    save_on_train_epoch_end=False,  # Allow saving during training steps\n",
    "    every_n_train_steps=checkpoint_every_n_steps,  # Now saves every n steps\n",
    "    save_top_k=3,  # Save all checkpoints (-1 means keep all)\n",
    "    monitor=\"train_loss\",\n",
    "    save_last=True,  # Save the last checkpoint\n",
    ")\n",
    "\n",
    "logger = TensorBoardLogger(log_dir, name=log_name)\n",
    "\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=epochs,\n",
    "    accelerator=\"auto\",\n",
    "    devices=2,\n",
    "    strategy=\"fsdp\",\n",
    "    callbacks=[\n",
    "        checkpoint_callback,\n",
    "    ],\n",
    "    accumulate_grad_batches=accumulate_grad_batches,\n",
    "    logger=logger,\n",
    "    precision=\"bf16-mixed\",\n",
    "    log_every_n_steps=1,\n",
    ")\n",
    "\n",
    "trainer.fit(pl_model, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deca8e73-1fc3-4474-a115-c7dace57ce12",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bc3686e-6a57-4200-b20a-61263e24380c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e6f5168-977e-47d7-8aac-b1b1b8fbcec3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a068a703-53dc-4c99-ad48-62273774eecb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bd3eb1b-64ca-496c-b468-71082b6cf1d5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, Trainer\n",
    "from modeling_deepseek import DeepseekV3ForCausalLM\n",
    "from datasets import load_dataset\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from datasets import load_dataset\n",
    "from memory_utils import count_parameters, memory_cleanup\n",
    "from transformers import BitsAndBytesConfig\n",
    "from ademamix import AdEMAMix\n",
    "from tqdm.auto import tqdm\n",
    "import json\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from bitsandbytes.optim import AdEMAMix8bit\n",
    "\n",
    "from liger_kernel.transformers import apply_liger_kernel_to_llama\n",
    "\n",
    "apply_liger_kernel_to_llama()\n",
    "\n",
    "# from Distiller import count_parameters\n",
    "torch.set_float32_matmul_precision(\"medium\")\n",
    "\n",
    "n_experts = 4\n",
    "n_active_experts = 1\n",
    "\n",
    "model_name = f\"DeepSeek-V3-{n_experts}@{n_active_experts}-unhealed-v0.1\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffa4184d-ddf7-4098-b979-1c33c5247c14",
   "metadata": {},
   "source": [
    "## Distribution strategy, with base weights on one gpu, and experts on the other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "317a46b4-5cac-4fde-a083-7dbf57f5881a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"{model_name}/model.safetensors.index.json\") as f:\n",
    "    weights_map = json.load(f)[\"weight_map\"]\n",
    "\n",
    "device_map = {}\n",
    "\n",
    "for elt in weights_map:\n",
    "    if \".layers.\" in elt:\n",
    "        device_map[elt] = \"cuda:1\"\n",
    "    else:\n",
    "        device_map[elt] = \"cuda:0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce387747-0043-4a5b-a4e1-ff703280dc65",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "bnb_config = BitsAndBytesConfig(\n",
    "    # load_in_8bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"fp4\",\n",
    "    bnb_4bit_quant_storage=torch.bfloat16,\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "\n",
    "model = DeepseekV3ForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    trust_remote_code=True,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    attn_implementation=\"flash_attention_2\",\n",
    "    device_map=device_map,  ## This should distribute automatically on all cpu\n",
    "    quantization_config=bnb_config,\n",
    ")\n",
    "\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "\n",
    "model.train()\n",
    "count_parameters(model)\n",
    "memory_cleanup()\n",
    "\n",
    "for name, parameter in model.named_parameters():\n",
    "    parameter.trainable = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77b58004-bc43-4d5f-9444-cd36695c6271",
   "metadata": {},
   "source": [
    "## Pushing Linear layer as 4bit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2620bcc2-57cc-4391-812f-52a638b20992",
   "metadata": {},
   "source": [
    "## Add lora layer on top of the experts and gate, freeze everything else"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6cdb127-d8b5-4e57-b0ad-b17297d3628e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "target_modules = []\n",
    "\n",
    "## Adapt only non shared experts\n",
    "for i in range(n_experts):\n",
    "    target_modules.append(f\"mlp.experts.{i}.gate_proj\")\n",
    "    target_modules.append(f\"mlp.experts.{i}.up_proj\")\n",
    "    target_modules.append(f\"mlp.experts.{i}.down_proj\")\n",
    "\n",
    "target_modules.append(\"mlp.gate.weight\")  ## Add all gate at once\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    # use_dora=True,\n",
    "    r=16,  # Rank of the low-rank matrices\n",
    "    lora_alpha=16,  # Scaling factor\n",
    "    target_modules=target_modules,\n",
    "    lora_dropout=0.1,  # Dropout rate\n",
    "    bias=\"none\",  # Whether to add bias\n",
    "    task_type=\"CAUSAL_LM\",  # Task type (Causal Language Modeling),\n",
    ")\n",
    "\n",
    "# Apply LoRA to the model\n",
    "model = get_peft_model(model, lora_config)\n",
    "count_parameters(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b3bfd67-9e5e-41dd-88b8-3e8246068f6a",
   "metadata": {},
   "source": [
    "## Loading the dataset for post training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f219999-6f92-483f-af1a-f8991b87141d",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_train = 65536\n",
    "# n_train=4096\n",
    "n_val = 256\n",
    "\n",
    "dolphin_r1 = load_dataset(\n",
    "    \"cognitivecomputations/dolphin-r1\",\n",
    "    \"nonreasoning\",\n",
    "    split=f\"train[:{n_train+n_val}]\",\n",
    "    cache_dir=\"../dolphin-r1\",\n",
    ")\n",
    "\n",
    "max_length = 64\n",
    "\n",
    "train_dataset = dolphin_r1.select_columns([\"messages\"]).select(list(range(n_train)))\n",
    "val_dataset = dolphin_r1.select_columns([\"messages\"]).select(list(range(n_train, n_train + n_val)))\n",
    "\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    formatted = tokenizer.apply_chat_template(examples[\"messages\"], tokenize=False, add_generation_prompt=False)\n",
    "    data = tokenizer(formatted, truncation=True, max_length=max_length, padding=True)\n",
    "    return data\n",
    "\n",
    "\n",
    "train_dataset = train_dataset.map(tokenize_function, batched=True, remove_columns=[\"messages\"]).with_format(\"torch\")\n",
    "val_dataset = val_dataset.map(tokenize_function, batched=True, remove_columns=[\"messages\"]).with_format(\"torch\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dbf6de5-26f9-46a2-9aa4-ce0bbc706d6c",
   "metadata": {},
   "source": [
    "## Set training hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d362b4c-1884-437f-9433-c955b0ac57ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_train_epochs = 1\n",
    "\n",
    "per_device_train_batch_size = 2\n",
    "per_device_eval_batch_size = 2\n",
    "\n",
    "gradient_accumulation_steps = 8  # Added gradient accumulation steps\n",
    "\n",
    "learning_rate = 1e-4\n",
    "\n",
    "weight_decay = 0.01\n",
    "logging_steps = 5\n",
    "seed = 3407\n",
    "\n",
    "output_dir = \"outputs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "804a9488-d555-4d0d-853d-3e94207ebf4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format datasets\n",
    "num_epochs = 1\n",
    "\n",
    "\n",
    "# model=torch.compile(model)\n",
    "train_dataset.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\"])\n",
    "val_dataset.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\"])\n",
    "\n",
    "# Create dataloaders\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=per_device_train_batch_size, shuffle=True)\n",
    "\n",
    "# Set up model and move to device\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "# Initialize optimizer\n",
    "optimizer = AdEMAMix8bit(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "# Initialize scheduler\n",
    "num_training_steps = len(train_dataloader) * num_epochs // gradient_accumulation_steps  # Adjusted num_training_steps\n",
    "scheduler = CosineAnnealingLR(optimizer, T_max=num_training_steps, eta_min=1e-6)\n",
    "\n",
    "# Initialize TensorBoard writer\n",
    "writer = SummaryWriter(f\"runs/{model_name}\")\n",
    "\n",
    "# Training loop\n",
    "model.train()\n",
    "global_step = 0\n",
    "\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    progress_bar = tqdm(train_dataloader, desc=f\"Epoch {epoch + 1}\")\n",
    "    running_loss = 0.0\n",
    "\n",
    "    optimizer.zero_grad()  # Initialize gradients to zero at the beginning of each accumulation step\n",
    "\n",
    "    for batch_idx, batch in enumerate(progress_bar):\n",
    "        # Move batch to device\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=input_ids)\n",
    "        loss = outputs.loss\n",
    "        loss = loss / gradient_accumulation_steps  # Normalize loss for gradient accumulation\n",
    "        loss.backward()\n",
    "\n",
    "        # Accumulate gradients and step optimizer every gradient_accumulation_steps\n",
    "        if (batch_idx + 1) % gradient_accumulation_steps == 0:\n",
    "            optimizer.step()\n",
    "            scheduler.step()  # Step the scheduler after optimizer step\n",
    "            optimizer.zero_grad()  # Reset gradients after optimizer step\n",
    "\n",
    "        # Update progress bar\n",
    "        running_loss += loss.item() * gradient_accumulation_steps  # Revert loss normalization for correct average loss\n",
    "        avg_loss = running_loss / (batch_idx + 1)\n",
    "        current_lr = scheduler.get_last_lr()[0]\n",
    "\n",
    "        progress_bar.set_postfix({\"loss\": f\"{avg_loss:.4f}\", \"lr\": f\"{current_lr:.2e}\"})\n",
    "\n",
    "        # Log metrics to TensorBoard\n",
    "        writer.add_scalar(\n",
    "            \"Training/Loss\", loss.item() * gradient_accumulation_steps, global_step\n",
    "        )  # Revert loss normalization for logging\n",
    "        writer.add_scalar(\"Training/Learning_Rate\", current_lr, global_step)\n",
    "\n",
    "        # Log every 10 steps (after accumulation steps)\n",
    "        if batch_idx % 10 == 0:\n",
    "            print(\n",
    "                f\"Step {batch_idx}, Loss: {loss.item() * gradient_accumulation_steps:.4f}, LR: {current_lr:.2e}\"\n",
    "            )  # Revert loss normalization for printing\n",
    "\n",
    "        global_step += 1\n",
    "\n",
    "    # Log epoch-level metrics\n",
    "    writer.add_scalar(\"Training/Epoch_Loss\", avg_loss, epoch)\n",
    "\n",
    "# Close TensorBoard writer\n",
    "writer.close()\n",
    "\n",
    "model.save_pretrained(f\"{model_name}-healing-lora\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bba6f40f-0520-46d3-b467-b5f653fc07a9",
   "metadata": {},
   "source": [
    "## Merge the unhealed model with its adapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afe752ee-e135-451f-8b9a-06addb0db54a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "from peft import PeftModel\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "peft_model_id = f\"{model_name}-healing-lora\"\n",
    "\n",
    "model = PeftModel.from_pretrained(base_model, peft_model_id, dtype=torch.bfloat16)\n",
    "model.merge_and_unload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ff66329-f479-4581-b71c-0430b5e00d82",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model_name = f\"DeepSeek-V3-{n_experts}@{n_active_experts}-Pruned\"\n",
    "model.save_pretrained(new_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4643165-0128-4685-84e5-de2b1efc05b6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.model.save_pretrained(\"deepseek_v2_lite_chat_16@4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "555c1593-f2be-449d-b89d-8df3a9dd361f",
   "metadata": {},
   "source": [
    "## Test generation capabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3f731e3-7a85-47cd-9b22-079186eb6832",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2239ebb9-b0c2-4fbd-a771-e9c8c06cdf88",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from transformers import TextStreamer\n",
    "\n",
    "streamer = TextStreamer(tokenizer, skip_prompt=True)\n",
    "\n",
    "model.generation_config.pad_token_id = model.generation_config.eos_token_id\n",
    "\n",
    "messages = [{\"role\": \"user\", \"content\": \"Write a piece of quicksort code in C++\"}]\n",
    "input_tensor = tokenizer.apply_chat_template(messages, add_generation_prompt=True, tokenize=False)\n",
    "input_tensor = tokenizer(input_tensor, return_tensors=\"pt\").to(\"cuda:0\")\n",
    "\n",
    "out = model.generate(**input_tensor, streamer=streamer, temperature=0.01, max_new_tokens=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5a03b61-1dcf-4db7-b941-bb432473d8d7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
