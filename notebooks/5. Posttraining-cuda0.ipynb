{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a5a03b61-1dcf-4db7-b941-bb432473d8d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c2d6e79c-2056-48b8-875f-d94b639d5667",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset\n",
    "from datasets import load_dataset\n",
    "from tqdm.auto import tqdm\n",
    "import bitsandbytes as bnb\n",
    "import torch\n",
    "import json\n",
    "import os\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoConfig,\n",
    "    AutoModelForCausalLM,\n",
    "    BitsAndBytesConfig,\n",
    ")\n",
    "\n",
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "\n",
    "from utils.ademamix import AdEMAMix\n",
    "from utils.config_utils import GenerationParams, PathConfig, DistillationParams\n",
    "from utils.adapters import DoRAAdapter\n",
    "from utils.torch_utils import (\n",
    "    save_quant,\n",
    "    load_quant,\n",
    "    destruct_module_optimized,\n",
    "    memory_cleanup,\n",
    "    get_nonreasoning_dataset,\n",
    "    load_weight,\n",
    "    rsetattr,\n",
    "    rgetattr,\n",
    "    load_weights,\n",
    "    rhasattr,\n",
    "    count_parameters\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d3f58eb8-7273-44ae-b229-deb01430bb3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class HealingDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, max_length=512):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.data[idx]\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            max_length=self.max_length,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        return {\n",
    "            \"input_ids\": encoding[\"input_ids\"].squeeze(),\n",
    "            \"attention_mask\": encoding[\"attention_mask\"].squeeze(),\n",
    "        }\n",
    "\n",
    "\n",
    "def load_and_prepare_data(tokenizer, batch_size=8, max_length=512, num_workers=os.cpu_count(), train_sample_limit=None, val_sample_limit=None):\n",
    "    dataset = load_dataset(\n",
    "        \"cognitivecomputations/dolphin-r1\", \"nonreasoning\", cache_dir=\"../dolphin-r1\"\n",
    "    )[\"train\"]\n",
    "\n",
    "    def filter_function(example):\n",
    "        if example[\"overall_quality\"] is not None and example[\"overall_quality\"] == 5:\n",
    "            return True\n",
    "        if example[\"score\"] is not None and example[\"score\"] >= 0.18:\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    dataset = dataset.filter(filter_function)\n",
    "    \n",
    "    # Apply sample limits if provided\n",
    "    if train_sample_limit is not None:\n",
    "        train_dataset = dataset.select(range(train_sample_limit))  # Use .select for efficiency\n",
    "    else:\n",
    "        train_dataset = dataset\n",
    "\n",
    "    train_dataset = train_dataset[\"messages\"]\n",
    "    \n",
    "    train_dataset = [\n",
    "        tokenizer.apply_chat_template(elt, tokenize=False, add_generation_prompt=False)\n",
    "        for elt in tqdm(train_dataset, desc=\"Preparing dataset train\")\n",
    "    ]\n",
    "    train_dataset = HealingDataset(\n",
    "        train_dataset, tokenizer, max_length=max_length\n",
    "    )\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers, pin_memory=True\n",
    "    )\n",
    "    return train_loader, None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "addaf657-e24d-4dd7-a233-48d7d2664f3e",
   "metadata": {},
   "source": [
    "## Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2d8a31eb-7b5e-4eaa-b86b-64ed4ae00bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_float32_matmul_precision('medium')\n",
    "\n",
    "device=\"cuda:0\"\n",
    "model_name=\"../deepseek_coder_v2_lite_instruct_awq\"\n",
    "n_epochs = 1\n",
    "start_layer = 1\n",
    "end_layer = 26\n",
    "target_routed_expert = 8\n",
    "target_active_expert = 4\n",
    "dora_rank = 16\n",
    "calibrate_merge=1\n",
    "calibrate_merge= calibrate_merge == 1\n",
    "pruning_method= \"act_cl\"\n",
    "\n",
    "path_config = PathConfig(\n",
    "    model_name = model_name,\n",
    "    intermediate_states = \"../data/intermediate_states\",\n",
    "    expert_states = \"../data/expert_states\",\n",
    "    expert_activations = \"../data/expert_activations\",\n",
    "    distillation_logs = \"../distillation_logs\",\n",
    "    moe_states=\"../moe_states\"\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "distillation_config = DistillationParams(\n",
    "    n_epochs= n_epochs,\n",
    "    target_routed_expert = target_routed_expert,\n",
    "    target_active_expert = target_active_expert,\n",
    "    eval_batches=16,\n",
    "    gradient_accumulation_steps= 4,\n",
    "    learning_rate= 3e-4,\n",
    "    end_factor= 0.2,\n",
    "    calibrate_merge=calibrate_merge,\n",
    "    skip_first_tokens=0, ## useful to avoid tuning on early tokens that have less informations\n",
    "    pruning_method=pruning_method, # topk , act_cl, state_cl\n",
    "    dora_rank=dora_rank,\n",
    ")\n",
    "\n",
    "unhealed_name=model_name+f\"_{distillation_config.pruning_method}_{distillation_config.target_routed_expert}a{distillation_config.target_active_expert}_{distillation_config.calibrate_merge}_{distillation_config.n_epochs}_unhealed\"\n",
    "unhealed_name=unhealed_name.replace('_awq', '')\n",
    "\n",
    "healed_name=unhealed_name.split('/')[-1].replace('_unhealed','')\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    unhealed_name, trust_remote_code=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbbb8530-6334-4b4f-8562-31e111673150",
   "metadata": {},
   "source": [
    "## Base Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "37014819-13e1-4033-896e-56edb32eca8b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a52a9eae0db14a5b9cfa4cf02166bb6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "quant_config = BitsAndBytesConfig(\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"fp4\",\n",
    "    bnb_4bit_quant_storage=torch.bfloat16,\n",
    ")\n",
    "\n",
    "with open(model_name+\"/model.safetensors.index.json\", \"r\") as f:\n",
    "    weight_map=json.loads(f.read())['weight_map']\n",
    "    \n",
    "model=AutoModelForCausalLM.from_pretrained(\n",
    "    unhealed_name,\n",
    "    device_map=device,\n",
    "    attn_implementation=\"flash_attention_2\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    trust_remote_code=True,\n",
    "    quantization_config=quant_config,\n",
    ")\n",
    "\n",
    "target_modules=[]\n",
    "for i in range(distillation_config.target_routed_expert):\n",
    "    target_modules.append(f\"mlp.experts.{i}.gate_proj\")\n",
    "    target_modules.append(f\"mlp.experts.{i}.up_proj\")\n",
    "    target_modules.append(f\"mlp.experts.{i}.down_proj\")\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    # use_dora=True,\n",
    "    target_modules=target_modules,\n",
    "    r=8,\n",
    "    lora_alpha=8,\n",
    "    lora_dropout=0.1\n",
    ")\n",
    "    \n",
    "model = get_peft_model(model, peft_config)\n",
    "\n",
    "for name, parameter in model.named_parameters():\n",
    "    if 'gate.weight' in name:\n",
    "        parameter.requires_grad=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "93b5d976-4005-4de3-9140-71e3a5ad595a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter Type       Count     \n",
      "==================== ==========\n",
      "Frozen Parameters    1,091,563,008\n",
      "Non-Frozen Parameters 17,678,336\n",
      "Total Parameters     1,109,241,344\n"
     ]
    }
   ],
   "source": [
    "count_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b09c8222-df73-41d7-a683-bfd9c5e0c826",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "efd63fddfe0640c495460a4c64d75b2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Preparing dataset train:   0%|          | 0/16000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47176b7f661443a49f71f3cfb42b693e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4000 [00:02<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torch\n",
    "from utils.ademamix import AdEMAMix\n",
    "from torch.optim.lr_scheduler import _LRScheduler\n",
    "import math\n",
    "\n",
    "class WarmupCosineAnnealingLR(_LRScheduler):\n",
    "    def __init__(self, optimizer, warmup_steps, total_steps, min_lr=0.0):\n",
    "        self.warmup_steps = warmup_steps\n",
    "        self.total_steps = total_steps\n",
    "        self.min_lr = min_lr\n",
    "        super(WarmupCosineAnnealingLR, self).__init__(optimizer, last_epoch=-1)\n",
    "\n",
    "    def get_lr(self):\n",
    "        if self.last_epoch < self.warmup_steps:\n",
    "            # Linear warmup phase\n",
    "            return [base_lr * (self.last_epoch / self.warmup_steps) for base_lr in self.base_lrs]\n",
    "        else:\n",
    "            # Cosine annealing phase\n",
    "            cosine_decay = 0.5 * (1.0 + math.cos(math.pi * (self.last_epoch - self.warmup_steps) / (self.total_steps - self.warmup_steps)))\n",
    "            decay_factor = (1 - self.min_lr) * cosine_decay + self.min_lr\n",
    "            return [base_lr * decay_factor for base_lr in self.base_lrs]\n",
    "            \n",
    "# Assuming model, tokenizer, and load_and_prepare_data are defined elsewhere\n",
    "\n",
    "total_steps = 32\n",
    "max_length = 512\n",
    "\n",
    "num_epochs=1\n",
    "num_sample = 16000\n",
    "\n",
    "batch_size = 4\n",
    "gradient_accumulation_steps = 2\n",
    "log_interval = gradient_accumulation_steps  # Log every 10 steps\n",
    "\n",
    "lr = 3e-4\n",
    "# Initialize the SummaryWriter\n",
    "writer = SummaryWriter(log_dir=f'runs/{healed_name}')\n",
    "\n",
    "train_loader, val_loader = load_and_prepare_data(\n",
    "    tokenizer, batch_size=batch_size, max_length=max_length,\n",
    "    train_sample_limit=num_sample, val_sample_limit=None\n",
    ")\n",
    "\n",
    "optimizer = AdEMAMix(\n",
    "    filter(lambda p: p.requires_grad, model.parameters()),\n",
    "    lr=lr,\n",
    "    betas=(0.9, 0.999, 0.9999),\n",
    "    alpha=8.0 #batch size is small so increazing alpha to smooth gradient\n",
    ")\n",
    "\n",
    "scheduler = WarmupCosineAnnealingLR(\n",
    "    optimizer,\n",
    "    warmup_steps=0,\n",
    "    total_steps=len(train_loader) // gradient_accumulation_steps,\n",
    "    min_lr=lr/50\n",
    ")\n",
    "\n",
    "# model=torch.compile(model)\n",
    "model.train()  # Ensure the model is in training mode\n",
    "\n",
    "for epoch in range(num_epochs):  # Assuming num_epochs is defined\n",
    "    for i, encoding in enumerate(tqdm(train_loader)):\n",
    "        input_ids = encoding['input_ids'].to(device)\n",
    "        attention_mask = encoding['attention_mask'].to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        output = model(\n",
    "            input_ids=input_ids,\n",
    "            labels=input_ids,  # Assuming labels are the same as input_ids for this task\n",
    "            attention_mask=attention_mask,\n",
    "            use_cache=False,\n",
    "            output_attentions=False,\n",
    "            output_hidden_states=False\n",
    "        )\n",
    "\n",
    "        # Compute loss and backpropagate\n",
    "        loss = output.loss\n",
    "        loss.backward()\n",
    "\n",
    "        # Update model parameters and learning rate\n",
    "        if (i + 1) % gradient_accumulation_steps == 0:\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        # Log loss and learning rate to TensorBoard\n",
    "        if (i + 1) % log_interval == 0:\n",
    "            global_step = epoch * len(train_loader) + i\n",
    "            writer.add_scalar('Loss/train', loss.item(), global_step)\n",
    "            writer.add_scalar('Learning Rate', scheduler.get_last_lr()[0], global_step)\n",
    "\n",
    "# Close the writer\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "57068478-54c0-4ba7-912f-fb4bed0e3055",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(healed_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cd3560d-1837-4b91-b04e-7745a5573cf3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2c8a4f57-4c70-4707-89f8-0d605ec12098",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `seen_tokens` attribute is deprecated and will be removed in v4.41. Use the `cache_position` model input instead.\n",
      "`get_max_cache()` is deprecated for all Cache classes. Use `get_max_cache_shape()` instead. Calling `get_max_cache()` will raise error from v4.48\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The color of Henri IV white horse was a brown, but it was not the perfect color in the eyes. However, the color was a mix of brown, red, and green, and it was slightly more than brown. The color of the horse was more than brown, but it was also slightly more than red, which is known as \"brown\" or \"red.\"\n",
      "\n",
      "The color of the horse was also slightly more than green, which is known as \"green\" or \"green.\" The color of the horse was slightly more than brown, but it was also slightly more than red. The color of the horse was also slightly more than green, which is known as \"green\" or \"green.\"\n",
      "\n",
      "The color of the horse was also slightly more than brown, but it was also slightly more than red. The color of the horse was also slightly more than green, which is known as \"green\" or \"green.\"\n",
      "\n",
      "The color of the horse was also slightly more than brown, but it was also slightly more than red. The color of the horse was also slightly more than green, which is known as \"green\" or \"green.\"\n",
      "\n",
      "The color of the horse was also slightly more than brown, but it was also slightly more than red. The color of the horse was also slightly more than green, which is known as \"green\" or \"green.\"\n",
      "\n",
      "The color of the horse was also slightly more than brown, but it was also slightly more than red. The color of the horse was also slightly more than green, which is known as \"green\" or \"green.\"\n",
      "\n",
      "The color of the horse was also slightly more than brown, but it was also slightly more than red. The color of the horse was also slightly more than green, which is known as \"green\" or \"green.\"\n",
      "\n",
      "The color of the horse was also slightly more than brown, but it was also slightly more than red. The color of the horse was also slightly more than green, which is known as \"green\" or \"green.\"\n",
      "\n",
      "The color of the horse was also slightly more than brown, but it was also slightly more than red. The color of the horse was also slightly more than green, which is known as \"green\" or \"green.\"\n",
      "\n",
      "The color of the horse was also slightly more than brown, but it was also slightly more than red. The color of the horse was also slightly more than green, which is known as \"green\" or \"green.\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import TextStreamer\n",
    "\n",
    "streamer = TextStreamer(tokenizer, skip_prompt=True)\n",
    "model.generation_config.pad_token_id = model.generation_config.eos_token_id\n",
    "\n",
    "prompt=\"\"\"Given the following evidences:\n",
    "- Henri IV was a famous king of france\n",
    "- Kings love to hunt and to joust\n",
    "- Hunting horse are always brown or camo\n",
    "- Camo is a pattern of color used to hise in plain sight\n",
    "\n",
    "Answer the following question:\n",
    "- What was the color of Henri IV white horse?\"\"\"\n",
    "\n",
    "messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "input_tensor = tokenizer.apply_chat_template(messages, add_generation_prompt=True, tokenize=False)\n",
    "input_tensor = tokenizer(input_tensor, return_tensors=\"pt\").to(device)\n",
    "\n",
    "out = model.generate(**input_tensor, streamer=streamer, temperature=0.3, repetition_penalty=1.01, max_new_tokens=512, do_sample=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1426019b-104a-4df0-a06d-002f686d23e3",
   "metadata": {},
   "source": [
    "distil_output\n",
    "\n",
    "Guacamole is a rich and flavorful creation of the Mexican cuisine. It's a mix of spices, fruits, and herbs. The perfect cocktail for the drink, its signature is: \"Guacamole is a mouth filled with a warm palette of flavor. It's a mix of spices, fruits, and herbs, like avocado, guilla, sour, guel, guam, guz, and guza. The flavors of this dish are intense, rich, vibrant, and sometimes you can cook with something that will you make it a more special, unforgettable, memorable, or unforgettable. The perfect cocktail is paired with a mix of citrus, fruity, sour, and savory, with the essence of spicy, smoky, and earthy flavors. It's a mouth filled with a warm palette of flavor, the taste is unique to create it as an authentic dish.\n",
    "I've got a little thing that I can cook with this dish. The ingredients are: a combination of tomatoes, potatoes, and carrots, sour, sour, sweet, and guol, gule, guza, guam, guz, and guza, or guel. It's a "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee96a9b2-3272-4b61-8d61-2c2c0eaa3238",
   "metadata": {},
   "source": [
    "nodist_corr\n",
    "\n",
    " The poem contains a poetic reflection of an experience of experimentation, experimentation, and experimentation with an mundane and mundane life of life. Guacamole is a poem that describes a world of culture,"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
