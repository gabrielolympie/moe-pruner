{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a5a03b61-1dcf-4db7-b941-bb432473d8d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c2d6e79c-2056-48b8-875f-d94b639d5667",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset\n",
    "from datasets import load_dataset\n",
    "from tqdm.auto import tqdm\n",
    "import bitsandbytes as bnb\n",
    "import torch\n",
    "import json\n",
    "import os\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoConfig,\n",
    "    AutoModelForCausalLM,\n",
    "    BitsAndBytesConfig,\n",
    ")\n",
    "\n",
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "\n",
    "# from utils.ademamix import AdEMAMix\n",
    "\n",
    "from bitsandbytes.optim.ademamix import AdEMAMix8bit as AdEMAMix\n",
    "\n",
    "from utils.config_utils import GenerationParams, PathConfig, DistillationParams\n",
    "from utils.adapters import DoRAAdapter\n",
    "from utils.torch_utils import (\n",
    "    save_quant,\n",
    "    load_quant,\n",
    "    destruct_module_optimized,\n",
    "    memory_cleanup,\n",
    "    get_nonreasoning_dataset,\n",
    "    load_weight,\n",
    "    rsetattr,\n",
    "    rgetattr,\n",
    "    load_weights,\n",
    "    rhasattr,\n",
    "    count_parameters\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d3f58eb8-7273-44ae-b229-deb01430bb3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class HealingDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, max_length=512):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.data[idx]\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            max_length=self.max_length,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        return {\n",
    "            \"input_ids\": encoding[\"input_ids\"].squeeze(),\n",
    "            \"attention_mask\": encoding[\"attention_mask\"].squeeze(),\n",
    "        }\n",
    "\n",
    "\n",
    "def load_and_prepare_data(tokenizer, batch_size=8, max_length=512, num_workers=os.cpu_count(), train_sample_limit=None, val_sample_limit=None):\n",
    "    # dataset = load_dataset(\n",
    "    #     \"cognitivecomputations/dolphin-r1\", \"nonreasoning\", cache_dir=\"../dolphin-r1\"\n",
    "    # )[\"train\"]\n",
    "\n",
    "    dataset = load_dataset(\n",
    "        \"HuggingFaceH4/ultrachat_200k\", \n",
    "    )[\"train_sft\"]\n",
    "\n",
    "    # def filter_function(example):\n",
    "    #     if example[\"overall_quality\"] is not None and example[\"overall_quality\"] == 5:\n",
    "    #         return True\n",
    "    #     if example[\"score\"] is not None and example[\"score\"] >= 0.16:\n",
    "    #         return True\n",
    "    #     return False\n",
    "\n",
    "    # dataset = dataset.filter(filter_function)\n",
    "    \n",
    "    # Apply sample limits if provided\n",
    "    if train_sample_limit is not None:\n",
    "        train_dataset = dataset.select(range(train_sample_limit))  # Use .select for efficiency\n",
    "    else:\n",
    "        train_dataset = dataset\n",
    "\n",
    "    train_dataset = train_dataset[\"messages\"]\n",
    "    \n",
    "    train_dataset = [\n",
    "        tokenizer.apply_chat_template(elt, tokenize=False, add_generation_prompt=False)\n",
    "        for elt in tqdm(train_dataset, desc=\"Preparing dataset train\")\n",
    "    ]\n",
    "    train_dataset = HealingDataset(\n",
    "        train_dataset, tokenizer, max_length=max_length\n",
    "    )\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers, pin_memory=True\n",
    "    )\n",
    "    return train_loader, None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "addaf657-e24d-4dd7-a233-48d7d2664f3e",
   "metadata": {},
   "source": [
    "## Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2d8a31eb-7b5e-4eaa-b86b-64ed4ae00bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_float32_matmul_precision('medium')\n",
    "\n",
    "device=\"cuda:0\"\n",
    "model_name=\"../deepseek_coder_v2_lite_instruct_awq\"\n",
    "n_epochs = 1\n",
    "start_layer = 1\n",
    "end_layer = 26\n",
    "target_routed_expert = 8\n",
    "target_active_expert = target_routed_expert\n",
    "dora_rank = 32\n",
    "calibrate_merge=1\n",
    "calibrate_merge= calibrate_merge == 1\n",
    "pruning_method= \"fused\"\n",
    "\n",
    "path_config = PathConfig(\n",
    "    model_name = model_name,\n",
    "    intermediate_states = \"../data/intermediate_states\",\n",
    "    expert_states = \"../data/expert_states\",\n",
    "    expert_activations = \"../data/expert_activations\",\n",
    "    distillation_logs = \"../distillation_logs\",\n",
    "    moe_states=\"../moe_states\"\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "distillation_config = DistillationParams(\n",
    "    n_epochs= n_epochs,\n",
    "    target_routed_expert = target_routed_expert,\n",
    "    target_active_expert = target_active_expert,\n",
    "    eval_batches=16,\n",
    "    gradient_accumulation_steps= 4,\n",
    "    learning_rate= 3e-4,\n",
    "    end_factor= 0.2,\n",
    "    calibrate_merge=calibrate_merge,\n",
    "    skip_first_tokens=0, ## useful to avoid tuning on early tokens that have less informations\n",
    "    pruning_method=pruning_method, # topk , act_cl, state_cl\n",
    "    dora_rank=dora_rank,\n",
    ")\n",
    "\n",
    "if distillation_config.pruning_method==\"progressive\":\n",
    "    unhealed_name=model_name+f\"_{distillation_config.pruning_method}_{distillation_config.target_routed_expert}a{distillation_config.target_active_expert}_unhealed\"\n",
    "elif distillation_config.pruning_method==\"fused\":\n",
    "    unhealed_name=model_name+f\"_fused_{distillation_config.target_routed_expert}_unhealed\"\n",
    "else:\n",
    "    unhealed_name=model_name+f\"_{distillation_config.pruning_method}_{distillation_config.target_routed_expert}a{distillation_config.target_active_expert}_{distillation_config.calibrate_merge}_{distillation_config.n_epochs}_unhealed\"\n",
    " \n",
    "unhealed_name=unhealed_name.replace('_awq', '')\n",
    "\n",
    "healed_name=unhealed_name.split('/')[-1].replace('_unhealed','')\n",
    "\n",
    "final_path=\"/home/golympie/ai-toolbox/pruned_models/\"\n",
    "final_name = os.path.join(final_path, unhealed_name.replace('_unhealed','').replace('../',''))\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    unhealed_name, trust_remote_code=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "57068478-54c0-4ba7-912f-fb4bed0e3055",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f466924dede4314834604d97a68185d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def prepare_limo(x):\n",
    "    return [\n",
    "        {'role':'user', 'content':x['question']},\n",
    "        {'role':'assistant', 'content':x['solution']},\n",
    "    ]\n",
    "    \n",
    "class HealingDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, max_length=512):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.data[idx]\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            max_length=self.max_length,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        return {\n",
    "            \"input_ids\": encoding[\"input_ids\"].squeeze(),\n",
    "            \"attention_mask\": encoding[\"attention_mask\"].squeeze(),\n",
    "        }\n",
    "\n",
    "\n",
    "def load_and_prepare_data(tokenizer, batch_size=8, max_length=512, num_workers=os.cpu_count(), train_sample_limit=None, val_sample_limit=None):\n",
    "    # dataset = load_dataset(\n",
    "    #     \"cognitivecomputations/dolphin-r1\", \"nonreasoning\", cache_dir=\"../dolphin-r1\"\n",
    "    # )[\"train\"]\n",
    "\n",
    "    dataset = load_dataset(\n",
    "        \"HuggingFaceH4/ultrachat_200k\", \n",
    "    )[\"train_sft\"]\n",
    "\n",
    "    dataset_refine = load_dataset(\n",
    "        \"GAIR/LIMO\", \n",
    "    )[\"train\"]\n",
    "\n",
    "    # def filter_function(example):\n",
    "    #     if example[\"overall_quality\"] is not None and example[\"overall_quality\"] == 5:\n",
    "    #         return True\n",
    "    #     if example[\"score\"] is not None and example[\"score\"] >= 0.16:\n",
    "    #         return True\n",
    "    #     return False\n",
    "\n",
    "    # dataset = dataset.filter(filter_function)\n",
    "    \n",
    "    # Apply sample limits if provided\n",
    "    if train_sample_limit is not None:\n",
    "        train_dataset = dataset.select(range(train_sample_limit))  # Use .select for efficiency\n",
    "    else:\n",
    "        train_dataset = dataset\n",
    "\n",
    "    train_dataset = train_dataset[\"messages\"]\n",
    "    \n",
    "    train_dataset = [\n",
    "        tokenizer.apply_chat_template(elt, tokenize=False, add_generation_prompt=False)\n",
    "        for elt in tqdm(train_dataset, desc=\"Preparing dataset train\")\n",
    "    ]\n",
    "\n",
    "    for elt in dataset_refine:\n",
    "        train_dataset.append(tokenizer.apply_chat_template(prepare_limo(elt), tokenize=False, add_generation_prompt=False)\n",
    "\n",
    "    train_dataset = HealingDataset(\n",
    "        train_dataset, tokenizer, max_length=max_length\n",
    "    )\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers, pin_memory=True\n",
    "    )\n",
    "    return train_loader, None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4af701b9-6e93-45a5-8f11-a7c2b4833a10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter Type       Count     \n",
      "==================== ==========\n",
      "Frozen Parameters    1,092,288,512\n",
      "Non-Frozen Parameters 225,785,344\n",
      "Total Parameters     1,318,073,856\n"
     ]
    }
   ],
   "source": [
    "count_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b2daffe-4bb2-4c2a-8f70-4538aa6b50df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7ea5ed020344624b40d09328625ab6a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Preparing dataset train:   0%|          | 0/16000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ecd62b3274a4db48975dc2ea5da25ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training, epoch 0:   0%|          | 0/8000 [00:01<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torch\n",
    "from utils.ademamix import AdEMAMix\n",
    "from torch.optim.lr_scheduler import _LRScheduler\n",
    "import math\n",
    "\n",
    "class WarmupCosineAnnealingLR(_LRScheduler):\n",
    "    def __init__(self, optimizer, warmup_steps, total_steps, min_lr=0.0):\n",
    "        self.warmup_steps = warmup_steps\n",
    "        self.total_steps = total_steps\n",
    "        self.min_lr = min_lr\n",
    "        super(WarmupCosineAnnealingLR, self).__init__(optimizer, last_epoch=-1)\n",
    "\n",
    "    def get_lr(self):\n",
    "        if self.last_epoch < self.warmup_steps:\n",
    "            # Linear warmup phase\n",
    "            return [base_lr * (self.last_epoch / self.warmup_steps) for base_lr in self.base_lrs]\n",
    "        else:\n",
    "            # Cosine annealing phase\n",
    "            cosine_decay = 0.5 * (1.0 + math.cos(math.pi * (self.last_epoch - self.warmup_steps) / (self.total_steps - self.warmup_steps)))\n",
    "            decay_factor = (1 - self.min_lr) * cosine_decay + self.min_lr\n",
    "            return [base_lr * decay_factor for base_lr in self.base_lrs]\n",
    "            \n",
    "# Assuming model, tokenizer, and load_and_prepare_data are defined elsewhere\n",
    "\n",
    "total_steps = 32\n",
    "max_length = 400\n",
    "\n",
    "num_epochs=1\n",
    "num_sample = 16000\n",
    "\n",
    "batch_size = 2\n",
    "gradient_accumulation_steps = 2\n",
    "log_interval = 1  # Log every 10 steps\n",
    "\n",
    "lr = 3e-4\n",
    "# Initialize the SummaryWriter\n",
    "writer = SummaryWriter(log_dir=f'runs/{healed_name}')\n",
    "\n",
    "train_loader, val_loader = load_and_prepare_data(\n",
    "    tokenizer, batch_size=batch_size, max_length=max_length,\n",
    "    train_sample_limit=num_sample, val_sample_limit=None\n",
    ")\n",
    "\n",
    "optimizer = AdEMAMix(\n",
    "    filter(lambda p: p.requires_grad, model.parameters()),\n",
    "    lr=lr,\n",
    "    betas=(0.9, 0.999, 0.9999),\n",
    "    alpha=5.0 #batch size is small so increazing alpha to smooth gradient\n",
    ")\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "    optimizer,\n",
    "    T_max=len(train_loader) // gradient_accumulation_steps,\n",
    "    eta_min=lr/50\n",
    ")\n",
    "\n",
    "# model=torch.compile(model, dynamic=True)\n",
    "model.train()  # Ensure the model is in training mode\n",
    "\n",
    "for epoch in range(num_epochs):  # Assuming num_epochs is defined\n",
    "    progress_bar = tqdm(train_loader, desc=f\"Training, epoch {epoch}\")\n",
    "    for i, encoding in enumerate(progress_bar):\n",
    "        input_ids = encoding['input_ids'].to(device)\n",
    "        attention_mask = encoding['attention_mask'].to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        output = model(\n",
    "            input_ids=input_ids,\n",
    "            labels=input_ids,  # Assuming labels are the same as input_ids for this task\n",
    "            attention_mask=attention_mask,\n",
    "            use_cache=False,\n",
    "            output_attentions=False,\n",
    "            output_hidden_states=False\n",
    "        )\n",
    "\n",
    "        # Compute loss and backpropagate\n",
    "        loss = output.loss\n",
    "        loss.backward()\n",
    "\n",
    "        # Update model parameters and learning rate\n",
    "        if (i + 1) % gradient_accumulation_steps == 0:\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        # Log loss and learning rate to TensorBoard\n",
    "\n",
    "        progress_bar.set_postfix(loss=loss.item())\n",
    "        \n",
    "        if (i + 1) % log_interval == 0:\n",
    "            global_step = epoch * len(train_loader) + i\n",
    "            writer.add_scalar('Loss/train', loss.item(), global_step)\n",
    "            writer.add_scalar('Learning Rate', scheduler.get_last_lr()[0], global_step)\n",
    "\n",
    "# Close the writer\n",
    "writer.close()\n",
    "model.save_pretrained(healed_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "531297bb-5a78-4950-808c-78d06a601c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(healed_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75d32620-6387-4f9e-9a88-d7393bd12fae",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d38e587-14b3-49d0-b0cb-b7f1f273ea20",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from peft import AutoPeftModelForCausalLM\n",
    "\n",
    "model=AutoPeftModelForCausalLM.from_pretrained(\n",
    "    healed_name,\n",
    "    device_map='cpu',\n",
    "    attn_implementation=\"flash_attention_2\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "model = model.merge_and_unload()\n",
    "\n",
    "model.save_pretrained(final_name)\n",
    "tokenizer.save_pretrained(final_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13928e6f-1adb-48b8-ba0a-e9d1ce87a2e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7659879a-5441-48a0-900c-bd5d9f54a34c",
   "metadata": {},
   "outputs": [],
   "source": [
    "shutil.copy(os.path.join('../patched_modules/', 'modeling_deepseek_fused_v2.py'), os.path.join(final_name, 'modeling_deepseek.py'))\n",
    "shutil.copy(os.path.join('../patched_modules/', 'configuration_deepseek_fused_v2.py'), os.path.join(final_name, 'configuration_deepseek.py'))\n",
    "\n",
    "shutil.copy(os.path.join('../patched_modules/', 'modeling_deepseek_fused_v2.py'), os.path.join(final_name, 'modeling_deepseek_fused_v2.py'))\n",
    "shutil.copy(os.path.join('../patched_modules/', 'configuration_deepseek_fused_v2.py'), os.path.join(final_name, 'configuration_deepseek_fused_v2.py'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b2fe477-e22b-4e00-b2d2-87fde868d69e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf0c8b94-f2ca-4457-bf32-c193090d4060",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(unhealed_name+\"/model.safetensors.index.json\", \"r\") as f:\n",
    "    weight_map=json.loads(f.read())['weight_map']\n",
    "    \n",
    "model=AutoModelForCausalLM.from_pretrained(\n",
    "    final_name,\n",
    "    device_map=device,\n",
    "    attn_implementation=\"flash_attention_2\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    trust_remote_code=True,\n",
    "    quantization_config=quant_config,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1daaba5-9c46-4c35-bf8e-843ac05dd92b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c8a4f57-4c70-4707-89f8-0d605ec12098",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TextStreamer\n",
    "\n",
    "streamer = TextStreamer(tokenizer, skip_prompt=True)\n",
    "model.generation_config.pad_token_id = model.generation_config.eos_token_id\n",
    "\n",
    "prompt=\"\"\"Given the following evidences:\n",
    "- Henri IV was a famous king of france\n",
    "- Kings love to hunt and to joust\n",
    "- Hunting horse are always brown or camo\n",
    "- Camo is a pattern of color used to hide in plain sight\n",
    "\n",
    "Answer the following question:\n",
    "- What was the color of Henri IV white horse?\"\"\"\n",
    "\n",
    "# prompt=\"Implement a basic snake game in python. Start your answer with ```python\"\n",
    "\n",
    "messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "input_tensor = tokenizer.apply_chat_template(messages, add_generation_prompt=True, tokenize=False)\n",
    "input_tensor = tokenizer(input_tensor, return_tensors=\"pt\").to(device)\n",
    "\n",
    "out = model.generate(**input_tensor, streamer=streamer, temperature=0.1, repetition_penalty=1.0, max_new_tokens=512, do_sample=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "790e23a7-72f5-4340-888f-e7d5e9f31864",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06cf0d91-1611-43fd-97b1-3339f5e1e780",
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55bc40a6-a824-41a2-becc-79e269d64699",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TextStreamer\n",
    "\n",
    "streamer = TextStreamer(tokenizer, skip_prompt=True)\n",
    "model.generation_config.pad_token_id = model.generation_config.eos_token_id\n",
    "\n",
    "prompt=\"\"\"Given the following evidences:\n",
    "- Henri IV was a famous king of france\n",
    "- Kings love to hunt and to joust\n",
    "- Hunting horse are always brown or camo\n",
    "- Camo is a pattern of color used to hide in plain sight\n",
    "\n",
    "Answer the following question:\n",
    "- What was the color of Henri IV white horse?\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "input_tensor = tokenizer.apply_chat_template(messages, add_generation_prompt=True, tokenize=False)\n",
    "input_tensor = tokenizer(input_tensor, return_tensors=\"pt\").to(device)\n",
    "\n",
    "out = model.generate(**input_tensor, streamer=streamer, temperature=0.01, repetition_penalty=1.0, max_new_tokens=512, do_sample=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1426019b-104a-4df0-a06d-002f686d23e3",
   "metadata": {},
   "source": [
    "distil_output\n",
    "\n",
    "Guacamole is a rich and flavorful creation of the Mexican cuisine. It's a mix of spices, fruits, and herbs. The perfect cocktail for the drink, its signature is: \"Guacamole is a mouth filled with a warm palette of flavor. It's a mix of spices, fruits, and herbs, like avocado, guilla, sour, guel, guam, guz, and guza. The flavors of this dish are intense, rich, vibrant, and sometimes you can cook with something that will you make it a more special, unforgettable, memorable, or unforgettable. The perfect cocktail is paired with a mix of citrus, fruity, sour, and savory, with the essence of spicy, smoky, and earthy flavors. It's a mouth filled with a warm palette of flavor, the taste is unique to create it as an authentic dish.\n",
    "I've got a little thing that I can cook with this dish. The ingredients are: a combination of tomatoes, potatoes, and carrots, sour, sour, sweet, and guol, gule, guza, guam, guz, and guza, or guel. It's a "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee96a9b2-3272-4b61-8d61-2c2c0eaa3238",
   "metadata": {},
   "source": [
    "nodist_corr\n",
    "\n",
    " The poem contains a poetic reflection of an experience of experimentation, experimentation, and experimentation with an mundane and mundane life of life. Guacamole is a poem that describes a world of culture,"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
