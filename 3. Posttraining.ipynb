{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5bd3eb1b-64ca-496c-b468-71082b6cf1d5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, Trainer\n",
    "from modeling_deepseek import DeepseekV3ForCausalLM\n",
    "from datasets import load_dataset\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from datasets import load_dataset\n",
    "from memory_utils import count_parameters, memory_cleanup\n",
    "from transformers import BitsAndBytesConfig\n",
    "from ademamix import AdEMAMix\n",
    "from tqdm.auto import tqdm\n",
    "import json\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter \n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from bitsandbytes.optim import AdEMAMix8bit\n",
    "\n",
    "from liger_kernel.transformers import apply_liger_kernel_to_llama\n",
    "\n",
    "apply_liger_kernel_to_llama()\n",
    "\n",
    "# from Distiller import count_parameters\n",
    "torch.set_float32_matmul_precision('medium')\n",
    "\n",
    "n_experts=4\n",
    "n_active_experts=1\n",
    "\n",
    "model_name = f\"DeepSeek-V3-{n_experts}@{n_active_experts}-unhealed\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffa4184d-ddf7-4098-b979-1c33c5247c14",
   "metadata": {},
   "source": [
    "## Distribution strategy, with base weights on one gpu, and experts on the other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "317a46b4-5cac-4fde-a083-7dbf57f5881a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"{model_name}/model.safetensors.index.json\") as f:\n",
    "    weights_map=json.load(f)['weight_map']\n",
    "\n",
    "device_map={}\n",
    "\n",
    "for elt in weights_map:\n",
    "    if \".layers.\" in elt:\n",
    "        device_map[elt]=\"cuda:1\"\n",
    "    else:\n",
    "        device_map[elt]=\"cuda:0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ce387747-0043-4a5b-a4e1-ff703280dc65",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n",
      "DeepseekV3ForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
      "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
      "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
      "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4708ce0090d413f9fabc49d4a87b216",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter Type       Count     \n",
      "==================== ==========\n",
      "Frozen Parameters    6,343,540,736\n",
      "Non-Frozen Parameters 1,856,027,880\n",
      "Total Parameters     8,199,568,616\n"
     ]
    }
   ],
   "source": [
    "bnb_config = BitsAndBytesConfig(\n",
    "    # load_in_8bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type='fp4',\n",
    "    bnb_4bit_quant_storage=torch.bfloat16,\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "\n",
    "model = DeepseekV3ForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    trust_remote_code=True,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    attn_implementation=\"flash_attention_2\",\n",
    "    device_map=device_map, ## This should distribute automatically on all cpu\n",
    "    quantization_config=bnb_config,\n",
    ")\n",
    "\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "\n",
    "\n",
    "model.train()\n",
    "count_parameters(model)\n",
    "memory_cleanup()\n",
    "\n",
    "for name, parameter in model.named_parameters():\n",
    "    parameter.trainable=False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77b58004-bc43-4d5f-9444-cd36695c6271",
   "metadata": {},
   "source": [
    "## Pushing Linear layer as 4bit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2620bcc2-57cc-4391-812f-52a638b20992",
   "metadata": {},
   "source": [
    "## Add lora layer on top of the experts and gate, freeze everything else"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f6cdb127-d8b5-4e57-b0ad-b17297d3628e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter Type       Count     \n",
      "==================== ==========\n",
      "Frozen Parameters    8,199,568,616\n",
      "Non-Frozen Parameters 102,629,376\n",
      "Total Parameters     8,302,197,992\n"
     ]
    }
   ],
   "source": [
    "target_modules=[]\n",
    "\n",
    "## Adapt only non shared experts\n",
    "for i in range(n_experts):\n",
    "    target_modules.append(f\"mlp.experts.{i}.gate_proj\")\n",
    "    target_modules.append(f\"mlp.experts.{i}.up_proj\")\n",
    "    target_modules.append(f\"mlp.experts.{i}.down_proj\")\n",
    "\n",
    "target_modules.append('mlp.gate.weight') ## Add all gate at once\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    # use_dora=True,\n",
    "    r=16,  # Rank of the low-rank matrices\n",
    "    lora_alpha=16,  # Scaling factor\n",
    "    target_modules=target_modules,\n",
    "    lora_dropout=0.1,  # Dropout rate\n",
    "    bias=\"none\",  # Whether to add bias\n",
    "    task_type=\"CAUSAL_LM\",  # Task type (Causal Language Modeling),\n",
    ")\n",
    "\n",
    "# Apply LoRA to the model\n",
    "model = get_peft_model(model, lora_config)\n",
    "count_parameters(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b3bfd67-9e5e-41dd-88b8-3e8246068f6a",
   "metadata": {},
   "source": [
    "## Loading the dataset for post training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3f219999-6f92-483f-af1a-f8991b87141d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b59526f37d77456c86a34672dc625522",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/256 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "n_train=65536\n",
    "# n_train=4096\n",
    "n_val=256\n",
    "\n",
    "dolphin_r1 = load_dataset(\n",
    "    'cognitivecomputations/dolphin-r1',\n",
    "    \"nonreasoning\",\n",
    "    split=f\"train[:{n_train+n_val}]\",\n",
    "    cache_dir=\"../dolphin-r1\"\n",
    ")\n",
    "\n",
    "max_length=64\n",
    "\n",
    "train_dataset = dolphin_r1.select_columns(['messages']).select(list(range(n_train)))\n",
    "val_dataset = dolphin_r1.select_columns(['messages']).select(list(range(n_train, n_train+n_val)))\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    formatted = tokenizer.apply_chat_template(examples['messages'], tokenize=False,add_generation_prompt=False)\n",
    "    data = tokenizer(formatted, truncation=True, max_length=max_length, padding=True)\n",
    "    return data\n",
    "\n",
    "train_dataset = train_dataset.map(tokenize_function, batched=True, remove_columns=['messages']).with_format(\"torch\")\n",
    "val_dataset = val_dataset.map(tokenize_function, batched=True, remove_columns=['messages']).with_format(\"torch\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dbf6de5-26f9-46a2-9aa4-ce0bbc706d6c",
   "metadata": {},
   "source": [
    "## Set training hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9d362b4c-1884-437f-9433-c955b0ac57ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "num_train_epochs = 1\n",
    "\n",
    "per_device_train_batch_size = 2\n",
    "per_device_eval_batch_size = 2\n",
    "\n",
    "gradient_accumulation_steps = 8 # Added gradient accumulation steps\n",
    "\n",
    "learning_rate = 1e-4\n",
    "\n",
    "weight_decay = 0.01\n",
    "logging_steps = 5\n",
    "seed = 3407\n",
    "\n",
    "output_dir = \"outputs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "804a9488-d555-4d0d-853d-3e94207ebf4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:   0%|                                                                                                                                     | 1/32768 [00:01<10:18:12,  1.13s/it, loss=9.8752, lr=1.00e-04]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0, Loss: 9.8752, LR: 1.00e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:   0%|                                                                                                                                     | 11/32768 [00:09<8:03:38,  1.13it/s, loss=9.7584, lr=1.00e-04]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 10, Loss: 10.2058, LR: 1.00e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:   0%|                                                                                                                                     | 21/32768 [00:18<7:52:59,  1.15it/s, loss=9.7222, lr=1.00e-04]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 20, Loss: 9.5637, LR: 1.00e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:   0%|▏                                                                                                                                    | 31/32768 [00:27<8:17:30,  1.10it/s, loss=9.3383, lr=1.00e-04]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 30, Loss: 7.3174, LR: 1.00e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:   0%|▏                                                                                                                                    | 41/32768 [00:36<8:03:35,  1.13it/s, loss=9.0567, lr=1.00e-04]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 40, Loss: 7.9860, LR: 1.00e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:   0%|▏                                                                                                                                    | 51/32768 [00:45<7:58:03,  1.14it/s, loss=8.8257, lr=1.00e-04]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 50, Loss: 8.4123, LR: 1.00e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:   0%|▏                                                                                                                                    | 61/32768 [00:54<8:01:18,  1.13it/s, loss=8.6373, lr=1.00e-04]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 60, Loss: 7.5023, LR: 1.00e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:   0%|▎                                                                                                                                    | 71/32768 [01:02<7:50:23,  1.16it/s, loss=8.5579, lr=1.00e-04]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 70, Loss: 9.6461, LR: 1.00e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:   0%|▎                                                                                                                                    | 81/32768 [01:11<8:23:58,  1.08it/s, loss=8.4379, lr=1.00e-04]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 80, Loss: 6.6156, LR: 1.00e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:   0%|▎                                                                                                                                    | 91/32768 [01:20<7:51:51,  1.15it/s, loss=8.2724, lr=1.00e-04]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 90, Loss: 6.7569, LR: 1.00e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:   0%|▍                                                                                                                                   | 101/32768 [01:28<7:48:16,  1.16it/s, loss=8.1183, lr=1.00e-04]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 100, Loss: 6.9941, LR: 1.00e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:   0%|▍                                                                                                                                   | 111/32768 [01:37<7:54:58,  1.15it/s, loss=8.0427, lr=1.00e-04]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 110, Loss: 7.6064, LR: 1.00e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:   0%|▍                                                                                                                                   | 121/32768 [01:46<7:49:55,  1.16it/s, loss=7.9150, lr=1.00e-04]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 120, Loss: 6.0329, LR: 1.00e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:   0%|▌                                                                                                                                   | 131/32768 [01:55<8:22:11,  1.08it/s, loss=7.7945, lr=1.00e-04]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 130, Loss: 6.3674, LR: 1.00e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:   0%|▌                                                                                                                                   | 141/32768 [02:03<7:47:14,  1.16it/s, loss=7.7441, lr=1.00e-04]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 140, Loss: 6.5364, LR: 1.00e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:   0%|▌                                                                                                                                   | 151/32768 [02:12<7:56:35,  1.14it/s, loss=7.6808, lr=1.00e-04]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 150, Loss: 6.4208, LR: 1.00e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:   0%|▋                                                                                                                                   | 161/32768 [02:21<7:58:49,  1.13it/s, loss=7.5836, lr=1.00e-04]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 160, Loss: 6.1026, LR: 1.00e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:   1%|▋                                                                                                                                   | 171/32768 [02:30<7:56:04,  1.14it/s, loss=7.5152, lr=1.00e-04]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 170, Loss: 6.5819, LR: 1.00e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:   1%|▋                                                                                                                                   | 181/32768 [02:39<8:19:38,  1.09it/s, loss=7.4560, lr=1.00e-04]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 180, Loss: 6.0976, LR: 1.00e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:   1%|▊                                                                                                                                   | 191/32768 [02:47<7:51:05,  1.15it/s, loss=7.3773, lr=1.00e-04]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 190, Loss: 4.6772, LR: 1.00e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:   1%|▊                                                                                                                                   | 201/32768 [02:56<7:55:55,  1.14it/s, loss=7.2878, lr=1.00e-04]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 200, Loss: 5.7742, LR: 1.00e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:   1%|▊                                                                                                                                   | 211/32768 [03:05<8:02:10,  1.13it/s, loss=7.2363, lr=1.00e-04]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 210, Loss: 5.5063, LR: 1.00e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:   1%|▉                                                                                                                                   | 221/32768 [03:14<7:55:03,  1.14it/s, loss=7.1727, lr=1.00e-04]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 220, Loss: 5.3915, LR: 1.00e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:   1%|▉                                                                                                                                   | 231/32768 [03:23<8:27:43,  1.07it/s, loss=7.1222, lr=1.00e-04]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 230, Loss: 5.1423, LR: 1.00e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:   1%|▉                                                                                                                                   | 241/32768 [03:32<7:59:00,  1.13it/s, loss=7.0452, lr=1.00e-04]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 240, Loss: 5.7270, LR: 1.00e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:   1%|█                                                                                                                                   | 251/32768 [03:40<7:49:23,  1.15it/s, loss=6.9943, lr=1.00e-04]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 250, Loss: 5.8168, LR: 1.00e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:   1%|█                                                                                                                                   | 261/32768 [03:49<7:57:38,  1.13it/s, loss=6.9375, lr=1.00e-04]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 260, Loss: 4.3878, LR: 1.00e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:   1%|█                                                                                                                                   | 271/32768 [03:58<7:53:53,  1.14it/s, loss=6.8793, lr=1.00e-04]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 270, Loss: 5.6801, LR: 1.00e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:   1%|█▏                                                                                                                                  | 281/32768 [04:07<8:06:04,  1.11it/s, loss=6.8237, lr=1.00e-04]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 280, Loss: 5.6726, LR: 1.00e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:   1%|█▏                                                                                                                                  | 291/32768 [04:16<8:06:27,  1.11it/s, loss=6.8000, lr=1.00e-04]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 290, Loss: 7.5208, LR: 1.00e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:   1%|█▏                                                                                                                                  | 301/32768 [04:25<8:02:23,  1.12it/s, loss=6.7550, lr=1.00e-04]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 300, Loss: 5.3307, LR: 1.00e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:   1%|█▎                                                                                                                                  | 311/32768 [04:34<8:06:32,  1.11it/s, loss=6.7044, lr=1.00e-04]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 310, Loss: 5.5986, LR: 1.00e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:   1%|█▎                                                                                                                                  | 321/32768 [04:43<8:07:39,  1.11it/s, loss=6.6547, lr=1.00e-04]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 320, Loss: 6.0684, LR: 1.00e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:   1%|█▎                                                                                                                                  | 331/32768 [04:52<8:21:55,  1.08it/s, loss=6.6118, lr=1.00e-04]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 330, Loss: 5.0333, LR: 1.00e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:   1%|█▎                                                                                                                                  | 341/32768 [05:01<8:02:36,  1.12it/s, loss=6.5675, lr=1.00e-04]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 340, Loss: 4.9092, LR: 1.00e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:   1%|█▍                                                                                                                                  | 351/32768 [05:10<7:56:25,  1.13it/s, loss=6.5172, lr=1.00e-04]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 350, Loss: 4.1396, LR: 1.00e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:   1%|█▍                                                                                                                                  | 361/32768 [05:19<8:09:42,  1.10it/s, loss=6.4983, lr=1.00e-04]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 360, Loss: 8.6138, LR: 1.00e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:   1%|█▍                                                                                                                                  | 371/32768 [05:28<7:58:51,  1.13it/s, loss=6.4544, lr=1.00e-04]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 370, Loss: 4.5429, LR: 1.00e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:   1%|█▌                                                                                                                                  | 381/32768 [05:37<8:01:16,  1.12it/s, loss=6.4090, lr=1.00e-04]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 380, Loss: 5.1284, LR: 1.00e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:   1%|█▌                                                                                                                                  | 391/32768 [05:46<7:58:12,  1.13it/s, loss=6.3826, lr=1.00e-04]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 390, Loss: 7.2647, LR: 1.00e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:   1%|█▌                                                                                                                                  | 401/32768 [05:55<8:04:45,  1.11it/s, loss=6.3428, lr=1.00e-04]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 400, Loss: 4.6516, LR: 1.00e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:   1%|█▋                                                                                                                                  | 411/32768 [06:04<8:09:10,  1.10it/s, loss=6.3120, lr=1.00e-04]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 410, Loss: 7.0964, LR: 1.00e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:   1%|█▋                                                                                                                                  | 421/32768 [06:13<8:00:30,  1.12it/s, loss=6.2867, lr=1.00e-04]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 420, Loss: 4.8728, LR: 1.00e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:   1%|█▋                                                                                                                                  | 431/32768 [06:22<7:56:51,  1.13it/s, loss=6.2508, lr=1.00e-04]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 430, Loss: 5.6109, LR: 1.00e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:   1%|█▊                                                                                                                                  | 441/32768 [06:31<8:05:28,  1.11it/s, loss=6.2218, lr=1.00e-04]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 440, Loss: 4.1152, LR: 1.00e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:   1%|█▊                                                                                                                                  | 451/32768 [06:40<7:57:47,  1.13it/s, loss=6.1877, lr=1.00e-04]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 450, Loss: 4.3511, LR: 1.00e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:   1%|█▊                                                                                                                                  | 461/32768 [06:49<8:18:54,  1.08it/s, loss=6.1579, lr=1.00e-04]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 460, Loss: 4.9827, LR: 1.00e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:   1%|█▉                                                                                                                                  | 471/32768 [06:58<7:57:31,  1.13it/s, loss=6.1326, lr=1.00e-04]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 470, Loss: 5.9913, LR: 1.00e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:   1%|█▉                                                                                                                                  | 481/32768 [07:07<8:03:09,  1.11it/s, loss=6.1013, lr=9.99e-05]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 480, Loss: 4.3230, LR: 9.99e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:   1%|█▉                                                                                                                                  | 491/32768 [07:16<8:02:14,  1.12it/s, loss=6.0678, lr=9.99e-05]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 490, Loss: 4.9407, LR: 9.99e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:   2%|██                                                                                                                                  | 501/32768 [07:25<7:59:34,  1.12it/s, loss=6.0347, lr=9.99e-05]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 500, Loss: 3.8420, LR: 9.99e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:   2%|██                                                                                                                                  | 511/32768 [07:34<8:18:32,  1.08it/s, loss=6.0072, lr=9.99e-05]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 510, Loss: 4.8715, LR: 9.99e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:   2%|██                                                                                                                                  | 521/32768 [07:43<8:02:21,  1.11it/s, loss=5.9804, lr=9.99e-05]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 520, Loss: 5.1575, LR: 9.99e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:   2%|██▏                                                                                                                                 | 531/32768 [07:52<7:57:26,  1.13it/s, loss=5.9424, lr=9.99e-05]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 530, Loss: 4.5310, LR: 9.99e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:   2%|██▏                                                                                                                                 | 541/32768 [08:01<8:03:29,  1.11it/s, loss=5.9176, lr=9.99e-05]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 540, Loss: 5.1231, LR: 9.99e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:   2%|██▏                                                                                                                                 | 551/32768 [08:10<7:56:15,  1.13it/s, loss=5.8902, lr=9.99e-05]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 550, Loss: 3.6745, LR: 9.99e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:   2%|██▎                                                                                                                                 | 561/32768 [08:19<8:36:40,  1.04it/s, loss=5.8636, lr=9.99e-05]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 560, Loss: 4.8074, LR: 9.99e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:   2%|██▎                                                                                                                                 | 571/32768 [08:28<7:57:48,  1.12it/s, loss=5.8445, lr=9.99e-05]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 570, Loss: 4.9775, LR: 9.99e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:   2%|██▎                                                                                                                                 | 574/32768 [08:32<7:58:47,  1.12it/s, loss=5.8380, lr=9.99e-05]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 50\u001b[0m\n\u001b[1;32m     48\u001b[0m loss \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mloss\n\u001b[1;32m     49\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss \u001b[38;5;241m/\u001b[39m gradient_accumulation_steps \u001b[38;5;66;03m# Normalize loss for gradient accumulation\u001b[39;00m\n\u001b[0;32m---> 50\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;66;03m# Accumulate gradients and step optimizer every gradient_accumulation_steps\u001b[39;00m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (batch_idx \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m%\u001b[39m gradient_accumulation_steps \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/torch/_tensor.py:626\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    616\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    617\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    618\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    619\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    624\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    625\u001b[0m     )\n\u001b[0;32m--> 626\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    627\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    628\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/torch/autograd/__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/torch/autograd/graph.py:823\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    821\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    822\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 823\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    824\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    825\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    826\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    827\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Format datasets\n",
    "num_epochs=1\n",
    "\n",
    "\n",
    "# model=torch.compile(model)\n",
    "train_dataset.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\"])\n",
    "val_dataset.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\"])\n",
    "\n",
    "# Create dataloaders\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=per_device_train_batch_size, shuffle=True)\n",
    "\n",
    "# Set up model and move to device\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "# Initialize optimizer\n",
    "optimizer = AdEMAMix8bit(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "# Initialize scheduler\n",
    "num_training_steps = len(train_dataloader) * num_epochs // gradient_accumulation_steps # Adjusted num_training_steps\n",
    "scheduler = CosineAnnealingLR(optimizer, T_max=num_training_steps, eta_min=1e-6)\n",
    "\n",
    "# Initialize TensorBoard writer\n",
    "writer = SummaryWriter(f'runs/{model_name}')\n",
    "\n",
    "# Training loop\n",
    "model.train()\n",
    "global_step = 0\n",
    "\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    progress_bar = tqdm(train_dataloader, desc=f\"Epoch {epoch + 1}\")\n",
    "    running_loss = 0.0\n",
    "\n",
    "    optimizer.zero_grad() # Initialize gradients to zero at the beginning of each accumulation step\n",
    "\n",
    "    for batch_idx, batch in enumerate(progress_bar):\n",
    "        # Move batch to device\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            labels=input_ids\n",
    "        )\n",
    "        loss = outputs.loss\n",
    "        loss = loss / gradient_accumulation_steps # Normalize loss for gradient accumulation\n",
    "        loss.backward()\n",
    "\n",
    "        # Accumulate gradients and step optimizer every gradient_accumulation_steps\n",
    "        if (batch_idx + 1) % gradient_accumulation_steps == 0:\n",
    "            optimizer.step()\n",
    "            scheduler.step() # Step the scheduler after optimizer step\n",
    "            optimizer.zero_grad() # Reset gradients after optimizer step\n",
    "\n",
    "\n",
    "        # Update progress bar\n",
    "        running_loss += loss.item() * gradient_accumulation_steps # Revert loss normalization for correct average loss\n",
    "        avg_loss = running_loss / (batch_idx + 1)\n",
    "        current_lr = scheduler.get_last_lr()[0]\n",
    "\n",
    "        progress_bar.set_postfix({\n",
    "            'loss': f'{avg_loss:.4f}',\n",
    "            'lr': f'{current_lr:.2e}'\n",
    "        })\n",
    "\n",
    "        # Log metrics to TensorBoard\n",
    "        writer.add_scalar('Training/Loss', loss.item() * gradient_accumulation_steps, global_step) # Revert loss normalization for logging\n",
    "        writer.add_scalar('Training/Learning_Rate', current_lr, global_step)\n",
    "\n",
    "        # Log every 10 steps (after accumulation steps)\n",
    "        if batch_idx % 10 == 0:\n",
    "            print(f'Step {batch_idx}, Loss: {loss.item() * gradient_accumulation_steps:.4f}, LR: {current_lr:.2e}') # Revert loss normalization for printing\n",
    "\n",
    "        global_step += 1\n",
    "\n",
    "    # Log epoch-level metrics\n",
    "    writer.add_scalar('Training/Epoch_Loss', avg_loss, epoch)\n",
    "\n",
    "# Close TensorBoard writer\n",
    "writer.close()\n",
    "\n",
    "model.save_pretrained(f'{model_name}-healing-lora')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bba6f40f-0520-46d3-b467-b5f653fc07a9",
   "metadata": {},
   "source": [
    "## Merge the unhealed model with its adapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afe752ee-e135-451f-8b9a-06addb0db54a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "from peft import PeftModel\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "peft_model_id = f'{model_name}-healing-lora'\n",
    "\n",
    "model = PeftModel.from_pretrained(\n",
    "    base_model,\n",
    "    peft_model_id,\n",
    "    dtype=torch.bfloat16\n",
    ")\n",
    "model.merge_and_unload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ff66329-f479-4581-b71c-0430b5e00d82",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model_name = f\"DeepSeek-V3-{n_experts}@{n_active_experts}-Pruned\"\n",
    "model.save_pretrained(new_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4643165-0128-4685-84e5-de2b1efc05b6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.model.save_pretrained('deepseek_v2_lite_chat_16@4')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "555c1593-f2be-449d-b89d-8df3a9dd361f",
   "metadata": {},
   "source": [
    "## Test generation capabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3f731e3-7a85-47cd-9b22-079186eb6832",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2239ebb9-b0c2-4fbd-a771-e9c8c06cdf88",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from transformers import TextStreamer\n",
    "streamer = TextStreamer(tokenizer, skip_prompt=True)\n",
    "\n",
    "model.generation_config.pad_token_id = model.generation_config.eos_token_id\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Write a piece of quicksort code in C++\"}\n",
    "]\n",
    "input_tensor = tokenizer.apply_chat_template(messages, add_generation_prompt=True, tokenize=False)\n",
    "input_tensor=tokenizer(input_tensor, return_tensors='pt').to('cuda:0')\n",
    "\n",
    "out = model.generate(\n",
    "    **input_tensor,\n",
    "    streamer=streamer,\n",
    "    temperature=0.01,\n",
    "    max_new_tokens=64\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5a03b61-1dcf-4db7-b941-bb432473d8d7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
