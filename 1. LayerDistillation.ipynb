{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d842eade-c8fd-4151-aa7c-90efbc113f3a",
   "metadata": {},
   "source": [
    "## Ressources"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85c5d3fe-04de-4a14-839b-07213c295466",
   "metadata": {},
   "source": [
    "The pipeline was optimized for the following config:\n",
    "- Storage : 2To SSD @512Mo/s\n",
    "- RAM : 128go DDR4 @3600\n",
    "- CPU : Ryzen 9 3950X 16@32 cores\n",
    "- GPU : 2x RTX 3090, aggregated 48gb DDR6X Vram\n",
    "\n",
    "Hence i can  not guarantee that it will work properly on more frugal hardware.\n",
    "Plus the GPU are not NVLink unified, so some optimisation involve manual allocation to one or the other GPU."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2b9e98c-9fc8-41d9-9f37-3a7a8170dbdf",
   "metadata": {},
   "source": [
    "## Compute the number of parameters for different pruning size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18bba445-5354-47ec-9c30-32ef1d8627a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_num_parameters(\n",
    "    n_routed_experts,\n",
    "    num_experts_per_tok,\n",
    "):\n",
    "    num_hidden_layers=61\n",
    "    first_k_dense_replace = 3\n",
    "    num_moe_layer = num_hidden_layers - first_k_dense_replace\n",
    "    \n",
    "    hidden_size=7168\n",
    "    intermediate_size=18432\n",
    "    moe_intermediate_size=2048\n",
    "    \n",
    "    \n",
    "    num_heads  = 128\n",
    "    q_lora_rank = 1536\n",
    "    qk_nope_head_dim = 128\n",
    "    qk_rope_head_dim = 64\n",
    "    kv_lora_rank = 512\n",
    "    v_head_dim=128\n",
    "    \n",
    "    n_shared_experts=1\n",
    "    \n",
    "    vocab_size = 129280\n",
    "    \n",
    "    gate_size = n_routed_experts * hidden_size\n",
    "    \n",
    "    mlp_weights = 3 * hidden_size * intermediate_size\n",
    "    moe_mlp_weights = 3 * hidden_size * moe_intermediate_size\n",
    "    \n",
    "    moe_total_weight = n_routed_experts * moe_mlp_weights\n",
    "    moe_active_weight = num_experts_per_tok * moe_mlp_weights\n",
    "    \n",
    "    q_head_dim = qk_nope_head_dim + qk_rope_head_dim\n",
    "    q_a_proj = hidden_size * q_lora_rank + q_lora_rank * q_head_dim\n",
    "    kv_a_proj_with_mqa = hidden_size * (kv_lora_rank  + qk_rope_head_dim) + kv_lora_rank * (num_heads * (q_head_dim - qk_rope_head_dim  + v_head_dim))\n",
    "    o_proj_weight = num_heads * v_head_dim * hidden_size\n",
    "    attention_weight = q_a_proj + 2 * kv_a_proj_with_mqa + o_proj_weight\n",
    "    \n",
    "    base_weight_per_moe_layer = attention_weight + n_shared_experts * moe_mlp_weights + gate_size\n",
    "    base_weight_per_mlp_layer = attention_weight + mlp_weights\n",
    "    \n",
    "    base_model_weight = base_weight_per_moe_layer * num_moe_layer + base_weight_per_mlp_layer * first_k_dense_replace + 2 * vocab_size * hidden_size\n",
    "    \n",
    "    total_expert_weight = moe_total_weight * num_moe_layer\n",
    "    active_expert_weight = moe_active_weight * num_moe_layer\n",
    "    \n",
    "    active_model_weight = active_expert_weight + num_moe_layer + base_model_weight\n",
    "    total_model_weight = total_expert_weight + num_moe_layer + base_model_weight\n",
    "    \n",
    "    print(f\"{n_routed_experts} @ {num_experts_per_tok} => {int(round(total_model_weight/1e9,0))}B @ {int(round(active_model_weight/1e9,0))}B parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5796530c-8061-4803-a7a2-077cac75ed0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_routed_experts=256\n",
    "num_experts_per_tok=8\n",
    "calc_num_parameters(n_routed_experts, num_experts_per_tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a54c46a-bbdb-4048-b616-d17e8c86b821",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = [\n",
    "    (256,8),\n",
    "    (22,6),\n",
    "    (16,4),\n",
    "    (8,2),\n",
    "    (4,1),\n",
    "]\n",
    "\n",
    "for elt in p:\n",
    "    calc_num_parameters(*elt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d69d425-c1fb-4289-b6e9-556e43edaae0",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "11abf472-db10-4106-9224-1ead46911ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import torch\n",
    "from accelerate import init_empty_weights\n",
    "from accelerate.utils import load_offloaded_weight\n",
    "import json\n",
    "from accelerate import load_checkpoint_in_model, dispatch_model\n",
    "from tqdm.auto import tqdm\n",
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "import gc\n",
    "import _pickle as pickle\n",
    "# from Distiller import MOEDistiller, count_parameters\n",
    "\n",
    "from transformers import BitsAndBytesConfig\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, GenerationConfig, AutoConfig\n",
    "\n",
    "import torch\n",
    "\n",
    "from memory_utils import load_module_weights_and_freeze_optimized, load_weight_cached, destruct_module_optimized\n",
    "from Distiller import load_model_config,create_empty_model,create_empty_layer\n",
    "\n",
    "from liger_kernel.transformers import apply_liger_kernel_to_llama\n",
    "\n",
    "apply_liger_kernel_to_llama()\n",
    "\n",
    "model_name = \"DeepSeek-V3\"\n",
    "offload_folder = model_name+'_offload/'\n",
    "output_directory = model_name+'_runner_output/'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"deepseek-ai/DeepSeek-V3\", trust_remote_code=True)\n",
    "\n",
    "def memory_cleanup():\n",
    "    \"\"\"Perform thorough memory cleanup\"\"\"\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.synchronize()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d6c541d-9c4a-4986-a90f-b787f7d614ef",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "## Create dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e402ac6c-0857-454c-806e-d9ce529834e2",
   "metadata": {},
   "source": [
    "The dataset is a small selection of the excellent dolphin r1 dataset because it contains both non reasoning and reasoning sample.\n",
    "\n",
    "Feel free to change the dataset or to scale the approach as you feel (will take longer but with better result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "598baa9f-5a41-4a33-a02f-fcea09e31515",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d493692be66b46f2a5aa70950bb406f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "batch_size=2\n",
    "\n",
    "n_batch=64 // batch_size\n",
    "n_sample=batch_size * n_batch\n",
    "\n",
    "gradient_accumulation_steps=8\n",
    "\n",
    "calibration = load_dataset(\n",
    "    'cognitivecomputations/dolphin-r1',\n",
    "    \"nonreasoning\",\n",
    "    cache_dir=\"../dolphin-r1\"\n",
    ")\n",
    "\n",
    "max_length=512\n",
    "calibration = calibration['train']\n",
    "position_ids = torch.arange(\n",
    "    0,\n",
    "    max_length,\n",
    "    dtype=torch.long,\n",
    "    device=\"cuda\",\n",
    ").unsqueeze(0)\n",
    "\n",
    "data=calibration['messages'][:n_sample]\n",
    "train_dataset = [tokenizer.apply_chat_template(elt, tokenize=False, add_generation_prompt=False) for elt in tqdm(data)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cf4b67a-9b91-481e-a1d1-d09e5f6547be",
   "metadata": {},
   "source": [
    "## Run the embedding layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee00a103-7772-405d-9972-322ea0fe66d3",
   "metadata": {},
   "source": [
    "Here we run the embedding layer on the dataset to build the first intermediate representations.\n",
    "This is not very vram intensive, but depending on the size of the dataset, it can quickly eat RAM as the tensors are very large.\n",
    "\n",
    "I was hesitant to quantise the representation for better memory optim, but was scared to lower the quality too much, especially in lower layers of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "285cea3d-6f8b-4f70-9716-7b28fa17eb92",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b205e556abc949e6b4ad9b4105d37983",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "from deepseek_v3.modeling_deepseek import _prepare_4d_causal_attention_mask\n",
    "import bitsandbytes as bnb\n",
    "\n",
    "base_path='/home/golympie/data/'\n",
    "\n",
    "## Load\n",
    "weight_map, config = load_model_config(\"deepseek_v3\")\n",
    "weight_file = weight_map['model.embed_tokens.weight']\n",
    "\n",
    "\n",
    "\n",
    "embed_tokens=torch.nn.Embedding(config.vocab_size, config.hidden_size, config.pad_token_id, device=\"cuda\")\n",
    "embed_tokens.weight.requires_grad=False\n",
    "embed_tokens.weight.copy_(load_weight_cached('model.embed_tokens.weight', weight_file, \"deepseek_v3\", \"cuda:0\"))\n",
    "\n",
    "\n",
    "intermediate = [0] * n_batch\n",
    "attention_masks = [0] * n_batch\n",
    "\n",
    "for batch_idx in tqdm(range(n_batch)):\n",
    "    batch=train_dataset[batch_idx * batch_size : (batch_idx + 1)*batch_size]\n",
    "    inputs = tokenizer(batch, max_length=max_length, padding=\"max_length\", truncation=True, return_tensors='pt').to('cuda')\n",
    "    embed =embed_tokens(inputs['input_ids'])\n",
    "\n",
    "    intermediate[batch_idx]=embed.to('cpu', dtype=torch.bfloat16)\n",
    "    \n",
    "destruct_module_optimized(embed_tokens)\n",
    "memory_cleanup()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd12cd68-4603-48f0-b01f-7798413915af",
   "metadata": {},
   "source": [
    "## Distill layers one by one"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a78bb18-46f2-453a-b4cc-bf7507b25978",
   "metadata": {},
   "source": [
    "Now the hard work, loading layers one by one to the GPUs and distill them.\n",
    "\n",
    "Note that a full layer in fp8 should take about 14 go vram during inference with batch size 2.\n",
    "The distiller makes a repartition of the pruned experts on remaining space. With the default config of this notebook, the first distillat is on cuda:0 and the other are in cuda 1.\n",
    "\n",
    "To make it work properly I had to make a custom implementation of FP8Linear layer, optimized for numerical stability, as the default bnb one was producing Nan outputs very frequently. The current implementation is a bit naive, and could probably be improved with custom triton kernels.\n",
    "\n",
    "The new layer was required as i am running the pipeline on Ampere GPU, and Deepseek released kernels can only work with Ada lovelace and plus generations.\n",
    "\n",
    "The logic is hardcoded in the Distiller file, and will need to be updated.\n",
    "\n",
    "With the current config, expect peak total vram usage of about 46 go. The pipeline should take about 24 hour on my setup to run, with a large part dedicated to disk write / read operations and quantization optimisations. If you have a larger vram, you can probably speed it up a bit :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b1e224c-47ee-4d04-bde6-238ef8eab198",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing modules: 100%|█████████████████████████████████████████████| 17/17 [00:08<00:00,  2.00it/s]\n",
      "Updating module structure: 100%|██████████████████████████████████| 14/14 [00:00<00:00, 114912.44it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba7ba14e956c4774a88801c7b9d3aa0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing modules: 100%|█████████████████████████████████████████████| 17/17 [00:08<00:00,  2.00it/s]\n",
      "Updating module structure: 100%|███████████████████████████████████| 14/14 [00:00<00:00, 87122.04it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd0df5875388406697f055e825aec243",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing modules: 100%|█████████████████████████████████████████████| 17/17 [00:08<00:00,  2.07it/s]\n",
      "Updating module structure: 100%|███████████████████████████████████| 14/14 [00:00<00:00, 67494.55it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c505a51bb6b48a2b74f6e9df3cccd83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing modules:  36%|███████████████▏                          | 469/1300 [02:09<04:02,  3.43it/s]"
     ]
    }
   ],
   "source": [
    "from copy import deepcopy\n",
    "from Distiller import MOEDistillerV3\n",
    "from copy import deepcopy\n",
    "from Distiller import MOEDistillerV3, count_parameters\n",
    "\n",
    "for i in range(65):\n",
    "    checkpoint_path=\"layers/\"\n",
    "    distilled_checkpoint_path=\"distilled_layers/\"\n",
    "    \n",
    "    os.makedirs(distilled_checkpoint_path, exist_ok=True)\n",
    "    \n",
    "    path = checkpoint_path+f\"layer{i}.ckpt\"\n",
    "    distilled_path= distilled_checkpoint_path+f\"layer{i}.ckpt\"\n",
    "    \n",
    "    layer = create_empty_layer(config, layer_idx=i)\n",
    "    layer = load_module_weights_and_freeze_optimized(\n",
    "        layer,\n",
    "        f\"model.layers.{i}\",\n",
    "        weight_map,\n",
    "        \"deepseek_v3\",\n",
    "        max_workers=32,\n",
    "        fp8_format=\"e4m3\",\n",
    "    )\n",
    "    memory_cleanup()\n",
    "        \n",
    "    if  \"DeepseekV3MLP\" in str(layer.mlp.__class__):\n",
    "        for batch_idx in tqdm(range(n_batch)):\n",
    "            with torch.amp.autocast('cuda'):\n",
    "                intermediate[batch_idx]= layer.forward(\n",
    "                    hidden_states=intermediate[batch_idx].to('cuda:0'),\n",
    "                    position_ids=position_ids,\n",
    "                )[0].detach().to('cpu')\n",
    "                if batch_idx % 100 == 0:\n",
    "                    memory_cleanup()\n",
    "        destruct_module_optimized(layer)\n",
    "        \n",
    "    else:\n",
    "        distiller = MOEDistillerV3(layer, i, model_name=model_name) # Example with accumulation\n",
    "        \n",
    "        calibration_batch=[elt.to('cuda:0', dtype=torch.bfloat16) for elt in intermediate[:128]]\n",
    "        # calibration_attention_batch=[_prepare_4d_causal_attention_mask(elt,(batch_size, max_length),embed,0).to('cuda:0') for elt in attention_masks[:4]]\n",
    "        \n",
    "        mlp_params = [\n",
    "            (22,6),\n",
    "            (16,4),\n",
    "            (8,2),\n",
    "            (4,1),\n",
    "        ]\n",
    "        \n",
    "        \n",
    "        for elt in mlp_params:\n",
    "            save_directory=model_name+f\"_{elt[0]}@{elt[1]}\"\n",
    "            os.makedirs(save_directory, exist_ok=True)\n",
    "        \n",
    "        distiller.calibrate(\n",
    "            calibration_batch,\n",
    "            position_ids,\n",
    "            mlp_params,\n",
    "            total_steps=n_batch,\n",
    "            gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "            learning_rate=1e-4,\n",
    "            temperature=2,\n",
    "        )\n",
    "        memory_cleanup()\n",
    "        \n",
    "        progress_bar = tqdm(range(n_batch), desc=\"Training\")\n",
    "        for batch_idx in progress_bar:\n",
    "            new_hidden_state, losses = distiller.step(\n",
    "                intermediate[batch_idx].to('cuda:0', dtype=torch.bfloat16),\n",
    "                attention_mask=None,\n",
    "                position_ids=position_ids,\n",
    "            )\n",
    "            intermediate[batch_idx]=new_hidden_state.cpu()\n",
    "            progress_bar.set_postfix(**losses)\n",
    "            memory_cleanup()\n",
    "\n",
    "        distiller.save_distillats()  # Call the save function\n",
    "        destruct_module_optimized(layer) # Destruct after saving\n",
    "        memory_cleanup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed7e1efb-5ebe-4789-9d98-dd9f517bd5cb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "distiller = MOEDistillerV3(layer, i, model_name=model_name) # Example with accumulation\n",
    "        \n",
    "calibration_batch=[elt.to('cuda:0', dtype=torch.bfloat16) for elt in intermediate[:16]]\n",
    "# calibration_attention_batch=[_prepare_4d_causal_attention_mask(elt,(batch_size, max_length),embed,0).to('cuda:0') for elt in attention_masks[:4]]\n",
    "\n",
    "mlp_params = [\n",
    "    (22,6),\n",
    "    (16,4),\n",
    "    (8,2),\n",
    "    (4,1),\n",
    "]\n",
    "\n",
    "\n",
    "for elt in mlp_params:\n",
    "    save_directory=model_name+f\"_{elt[0]}@{elt[1]}\"\n",
    "    os.makedirs(save_directory, exist_ok=True)\n",
    "\n",
    "distiller.calibrate(\n",
    "    calibration_batch,\n",
    "    position_ids,\n",
    "    mlp_params,\n",
    "    total_steps=n_batch,\n",
    "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "    learning_rate=1e-4,\n",
    "    temperature=2,\n",
    ")\n",
    "memory_cleanup()\n",
    "\n",
    "progress_bar = tqdm(range(16), desc=\"Training\")\n",
    "for batch_idx in progress_bar:\n",
    "    new_hidden_state, losses = distiller.step(\n",
    "        intermediate[batch_idx].to('cuda:0', dtype=torch.bfloat16),\n",
    "        attention_mask=None,\n",
    "        position_ids=position_ids,\n",
    "    )\n",
    "    intermediate[batch_idx]=new_hidden_state.cpu()\n",
    "    progress_bar.set_postfix(**losses)\n",
    "    memory_cleanup()\n",
    "\n",
    "distiller.save_distillats()  # Call the save function\n",
    "destruct_module_optimized(layer) # Destruct after saving\n",
    "memory_cleanup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6589826-b3e0-4b09-ae5b-f2331bec35fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = create_empty_layer(config, layer_idx=i)\n",
    "layer = load_module_weights_and_freeze_optimized(\n",
    "    layer,\n",
    "    f\"model.layers.{i}\",\n",
    "    weight_map,\n",
    "    \"deepseek_v3\",\n",
    "    max_workers=32,\n",
    "    fp8_format=\"e4m3\",\n",
    ")\n",
    "memory_cleanup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b991744-a403-494a-a098-c3f5118c7ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory_cleanup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "520a4407-368a-4062-87ec-81f9ca6b8bc8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x = distiller.distillats[0]['moe'].experts[0].gate_proj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2be1809-f737-4941-bc2c-2ec75c1e1a62",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = x.merge_and_unload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23aeeab5-89d3-4e13-b56e-477e72ae7589",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
